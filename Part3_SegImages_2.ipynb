{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part3_SegImages_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "bgRTIeemTfoY"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmobley/liveproject_water/blob/master/Part3_SegImages_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWUCy6h3RvAx"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<tr style=\"vertical-align: top; padding: 0; margin: 0;background-color: #ffffff\">\n",
        "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
        "    <p style=\"background: #182AEB; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
        "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #ffffff;\">Deep Learning </span> for Satellite Image Classification (Manning Publications)</span><br/>by <em>Daniel Buscombe</em> </strong><br/><br/>\n",
        "        <strong>> Chapter 3: Image Segmentation Models </strong><br/>\n",
        "    </p>           \n",
        "\n",
        "<p style=\"border: 1px solid #182AEB; border-left: 15px solid #182AEB; padding: 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #182AEB\">What you learned in Part 2.</strong>  \n",
        "    <br/>In Part 2, you developed a number of datasets to train and test segmentation models: <br/> 1) An image set consisting of time-series of cloudless sentinel-2 imagery at a number of lakes undergoing change; <br/> 2) A subset of representative sample images and a corresponding label image set and label images for each sample image; <br/> 3) An image set consisting of NWPU-RESISC45 data consisting of 700-example satellite images each of 45 land cover/use classes, including lakes; and <br/> 4) A representative subset of NWPU-RESISC45 lake images and a corresponding label image set.\n",
        "    </p>\n",
        "    \n",
        "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #ff5733\">What you will learn in this Part.</strong>  \n",
        "    <br/>In Part 3, you will first learn how to create image generators, and how to create an augmented data set of the above images and labels, consisting of the original set plus a set of randomly generated versions of the originals that have been transformed in some way. Then, you will learn how to build the deep convolutional autoencoder-decoder “U-Net” model to perform semantic segmentation of imagery. \n",
        "    </p>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E4FqG_lWPIg"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>Introduction to <br/> Google Colab</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "When open in colaboratory, go to **Runtime** on the toolbar, then **Change runtime type**, and finally under **Hardware accelerator*** select **GPU** and hit **Save**. \n",
        "\n",
        "This will allow you to train models using a GPU, which is a necessity if you want model training to complete in a reasonable time. \n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
        "The following experiments are compute-heavy (large model and dataset). Make sure the procedures are performed on GPU(s)\n",
        "</p>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrITBcKDRvA1"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>Preparing datasets for <br/> a deep learning project</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "This Part introduces several ways to generate satellite imagery and associated labels that you can use in your deliverables. We will use these data to perform a semantic segmentation of imagery to identify water pixels. Semantic segmentation is a type of <b>classification</b> algorithm.\n",
        "    In machine (deep) learning with imagery, a classification algorithm is usually one that is used to answer binary yes-or-no questions (e.g. \"Does this image contain water?\") or a multiclass classification (\"Which of the following is in this image: water, grass, trees?\"). Supervised classification is when you also provide the right answers or `labels', so an algorithm can learn from them. In semantic segmentation, each pixel is classified so labels are images where each pixel is a code that corresponds to a particular class. In our binary segmentation task, label images are composed of 1 for water and 0 for everything else. \n",
        " \n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
        "    <a href=\"https://machinelearningmastery.com/difference-test-validation-datasets/\">Read this guide to dataset splits:</a>\n",
        "<ul>\n",
        "  <li>Training dataset: The dataset that we use to train the model (weights and biases of the neural network). The model sees and learns from this data.</li>    \n",
        "  <li>Test dataset: The dataset that is used to evaluate the final trained model.</li>\n",
        "</ul>    \n",
        "</p>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZpW26HHv0wz"
      },
      "source": [
        "#### Copying the data: Colab users\n",
        "\n",
        "Import tensorflow, check the version, and make sure you have a GPU at your disposal\n",
        "\n",
        "Uncomment and run the cell below to ensure you have TensorFlow 2 installed on you colab environment (if that is how you are running this notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh-qKV3MLz5g"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QTVJS7xj0DC"
      },
      "source": [
        "#colab = 0\n",
        "colab = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UqQOGkZZgP-"
      },
      "source": [
        "if colab==1:\n",
        "    !pip install --default-timeout=1000 tensorflow-gpu==2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP4hQeYDcNvh"
      },
      "source": [
        "You may have to restart the runtime and/or change runtime type here, if the following doesn't show Tensorflow version 2, and a GPU available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5eqSezGZuy_"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLR7TM7DZz04"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAXBEbZMwZbX"
      },
      "source": [
        "print(tf.test.is_gpu_available())\n",
        "#if colab == 1:\n",
        "#  print(tf.config.list_physical_devices('GPU'))\n",
        "#else:\n",
        "#  print(tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPC_PyQfwbJw"
      },
      "source": [
        "If you are running this notebook on Colab, you may find the following workflow convenient, whereby images and labels are downloaded from google drive an unzipped, like we saw in Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lJ-SEKnR24D"
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination, use_stream=True):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = use_stream)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = use_stream)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-IG3PHHwcbY"
      },
      "source": [
        "Uncomment the ```download_file_from_google_drive``` to actually download the zipped files into Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlY3Qav3R29r"
      },
      "source": [
        "#imagery\n",
        "file_id = '1iMfIjr_ul49Ghs2ewazjCt8HMPfhY47h'\n",
        "destination = 's2cloudless_imagery.zip'\n",
        "if colab==1:\n",
        "    download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k24kKz6_R3E4"
      },
      "source": [
        "#labels\n",
        "file_id = '1c7MpwKVejoUuW9F2UaF_vps8Vq2RZRfR'\n",
        "destination = 's2cloudless_label_imagery.zip'\n",
        "if colab==1:\n",
        "    download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF4q0433R3C_"
      },
      "source": [
        "import zipfile\n",
        "def unzip(f):\n",
        "    \"\"\"\n",
        "    f = file to be unzipped\n",
        "    \"\"\"    \n",
        "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
        "        zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpfrch4BwdUw"
      },
      "source": [
        "Again, for Colab users, uncomment the unzip commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJnhMsQAR3Ap"
      },
      "source": [
        "if colab==1:\n",
        "    unzip('s2cloudless_imagery.zip')\n",
        "    unzip('s2cloudless_label_imagery.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zT6inQFreY0"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgRTIeemTfoY"
      },
      "source": [
        "##### Other users (local desktop/laptop)\n",
        "\n",
        "Copy the ```s2cloudless_imagery``` directory over from ```2_Data```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcLddQPETfoa"
      },
      "source": [
        "if colab!=1:\n",
        "    import shutil, os\n",
        "    shutil.copytree('..'+os.sep+'2_Data'+os.sep+'s2cloudless_imagery', 's2cloudless_imagery')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hP7BTEITfoe"
      },
      "source": [
        "Copy the ```s2cloudless_label_imagery``` directory over from ```2_Solution```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZWJhlx3Tfog"
      },
      "source": [
        "if colab!=1:\n",
        "    shutil.copytree('..'+os.sep+'2_Solution'+os.sep+'s2cloudless_label_imagery', 's2cloudless_label_imagery')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp95SM62TfpV"
      },
      "source": [
        "Finally, copy over the labels json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTNU-R7CTfpW"
      },
      "source": [
        "if colab!=1:\n",
        "    shutil.copy('..'+os.sep+'2_Data'+os.sep+'s2cloudless_labels'+os.sep+'all_labels.json', 'all_labels.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kedge5unTfp6"
      },
      "source": [
        "Optionally, you could now delete the original directories to save space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CoIiA-Ewes3"
      },
      "source": [
        "#### Image Generator Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X--3YHtwTfp8"
      },
      "source": [
        "When training machine learning models on large data sets, it is often optimal to generate your dataset in real time and feed it right away to your deep learning model while it is training.\n",
        "\n",
        "We do that using generators. Read up on generators in python [here](https://realpython.com/introduction-to-python-generators/). The generator will _generate_ a batch of random images and associated masks, which is fed to the neural network at the start of a training epoch. Much more about training in the next Part. For now, we're concerned only with making that generator function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIXZ0VVCRvA3"
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json, os, glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhm-AabowgQE"
      },
      "source": [
        "The next function creates a generator that will randomly draw images from the training and testing sets. The inputs:\n",
        "\n",
        "* files = list of image files\n",
        "* batch_size = 32 = number of images (plus corresponding labels) to use in each batch \n",
        "* sz = (512, 512) = image size to use. Typically we want relatively small images for computational efficiency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6B_BmZ1RvA8"
      },
      "source": [
        "def image_batch_generator(files, batch_size = 32, sz = (512, 512)):\n",
        "  \n",
        "  while True: # this is here because it will be called repeatedly by the training function\n",
        "    \n",
        "    #extract a random subset of files of length \"batch_size\"\n",
        "    batch = np.random.choice(files, size = batch_size)    \n",
        "    \n",
        "    #variables for collecting batches of inputs (x) and outputs (y)\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    \n",
        "    #cycle through each image in the batch\n",
        "    for f in batch:\n",
        "\n",
        "        #preprocess the raw images \n",
        "        rawfile = f's2cloudless_imagery/data/{f}'\n",
        "        raw = Image.open(rawfile)\n",
        "        raw = raw.resize(sz)\n",
        "        raw = np.array(raw)\n",
        "\n",
        "        #check the number of channels because some of the images are RGBA or GRAY\n",
        "        if len(raw.shape) == 2:\n",
        "            raw = np.stack((raw,)*3, axis=-1)\n",
        "\n",
        "        else:\n",
        "            raw = raw[:,:,0:3]\n",
        "            \n",
        "        #get the image dimensions, find the min dimension, then square the image off    \n",
        "        nx, ny, nz = np.shape(raw)\n",
        "        n = np.minimum(nx,ny)\n",
        "        raw = raw[:n,:n,:] \n",
        "            \n",
        "        batch_x.append(raw)\n",
        "        \n",
        "        #get the masks. \n",
        "        maskfile = rawfile.replace('s2cloudless_imagery','s2cloudless_label_imagery')+'_mask.jpg'\n",
        "        mask = Image.open(maskfile)\n",
        "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
        "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
        "        # water pixels are always greater than 100\n",
        "        mask = (mask>100).astype('int')\n",
        "        \n",
        "        mask = mask[:n,:n]\n",
        "\n",
        "        batch_y.append(mask)\n",
        "\n",
        "    #preprocess a batch of images and masks \n",
        "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
        "    batch_y = np.array(batch_y)\n",
        "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
        "\n",
        "    yield (batch_x, batch_y) #yield both the image and the label together"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K7glEp4TfqD"
      },
      "source": [
        "Get a list of images and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uEk1DXhceNf"
      },
      "source": [
        "Colab users, you will need to quickly upload your copy of ```all_labels.json``` here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "terFaWtDswSA"
      },
      "source": [
        "if colab == 1:\n",
        "  file_id = '1Zz7scOC5rc8VmO1_vx7BzCJP0BjHH9mX'\n",
        "  destination = 'all_labels.json'\n",
        "  download_file_from_google_drive(file_id, destination)\n",
        "  #with open(\"all_labels.json\",\"rt\") as myf:\n",
        "  #  data = myf.read()\n",
        "  #  print(data)\n",
        "  data = json.load(open(\"all_labels.json\",\"rt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnsAFZkYTfqE"
      },
      "source": [
        "# Very tedious to have to do this every time I run, but could not\n",
        "# get above code to work. Some issue with the decoding that I will\n",
        "# need to investigate someday.\n",
        "#if colab == 1:\n",
        "#  from google.colab import files\n",
        "#  uploaded = files.upload()\n",
        "#  data = json.loads(uploaded['all_labels.json'].decode(\"utf-8\"))\n",
        "#else:\n",
        "#  data = json.load(open('all_labels.json'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HplgowBFTfqI"
      },
      "source": [
        "# get a sorted list of image filenames, which are the 'keys' of the dictionary object\n",
        "images = sorted(data.keys())\n",
        "#data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7XX9YWmjcdx"
      },
      "source": [
        "%ls\n",
        "#rm all_labels\\ \\(1\\).json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvkjjsuoRvBH"
      },
      "source": [
        "# use glob to find all jpg files in the imagery folder\n",
        "labels = sorted(glob.glob('s2cloudless_label_imagery'+os.sep+'data'+os.sep+'*.jpg'))\n",
        "# remove the subdirectory suffix from each label\n",
        "labels = [l.split(os.sep)[-1] for l in labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1HvI9ZGTfqV"
      },
      "source": [
        "#### Setting up model training hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYItqE_F2CLn"
      },
      "source": [
        "Next we set our batch size (number of images to pass to each model training epoch, 5), and the proportion of all images to use for training (0.5, or 50%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd4kJ6sjRvBP"
      },
      "source": [
        "batch_size = 5\n",
        "prop_train = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oXgh-MLTfqZ"
      },
      "source": [
        "We define train and test splits simply as the first and second halves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt9FdOvNTfqa"
      },
      "source": [
        "split = int(prop_train * len(images))\n",
        "\n",
        "#split into training and testing\n",
        "train_files = images[0:split]\n",
        "test_files  = images[split:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMkIZ5bF2WS0"
      },
      "source": [
        "The next few lines randomly select a few images and display them and their associated masks, and the resulting masked images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H0QemnhRvBR"
      },
      "source": [
        "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
        "test_generator  = image_batch_generator(test_files, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BySPnnuwTfqh"
      },
      "source": [
        "To access the ```next``` pair of image and label from the generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEiMZFMyRvBU"
      },
      "source": [
        "x, y= next(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WTMOGilTfqm"
      },
      "source": [
        "Make a function to generate an image for display, composed of the original image, binary mask, and masked image side-by-side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmayH5ucRvBW"
      },
      "source": [
        "def get_pair(i):\n",
        "    img = x[i]\n",
        "    msk = y[i].squeeze()\n",
        "    msk = np.stack((msk,)*3, axis=-1)\n",
        "    return np.concatenate([img, msk, img*msk], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ippmsWGaTfqt"
      },
      "source": [
        "Show the first five images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "343FE2kSRvBY"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(0))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(1))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(2))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(3))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_Xdd-GhKgD"
      },
      "source": [
        "in the next Part, you will see how we can use the ```image_batch_generator``` function above to get separate generators for training and testing subsets of the data, and pass each generator to a keras model training session. Given a ```batch_size``` and list of ```test``` and ```train``` filenames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI0a9kQPhKs6"
      },
      "source": [
        "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
        "test_generator  = image_batch_generator(test_files, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzsXX2Y-RvCG"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>Image augmentation</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "Deep learning models usually need a lot of data to be properly trained. It is often useful to get more data from the existing ones using data augmentation techniques. The main ones are summed up in the table below. More precisely, given the following input image, here are some of the techniques that we can apply:\n",
        "\n",
        "* Flip (horizontal or vertical).\n",
        "* Rotation. \n",
        "* Brightness.  \n",
        "* Information loss\n",
        "* Random cropping (subsets of images)\n",
        "* Contrast\n",
        "* Random zoom\n",
        "* etc\n",
        "\n",
        "</p>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbrjxWeMc0sx"
      },
      "source": [
        "For consistency with previous Parts we will again use rasterio to read in image files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3uLGiSYVjuf"
      },
      "source": [
        "if colab==1:\n",
        "    !pip install rasterio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWr0nDBPTfqz"
      },
      "source": [
        "import json, os\n",
        "import glob\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxeXjAjTc8EM"
      },
      "source": [
        "#### Principles and rationale of data augmentation and augmenting imagery with keras/Tensorflow 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTDXYyQZTfq4"
      },
      "source": [
        "The keras ```ImageDataGenerator``` function is extremely useful for implementing several common image augmentation strategies, for example the below would randomly apply zoom, shear, rotation, shifts, and requantization of the imagery "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfuyJJH6Tfq5"
      },
      "source": [
        "im_gen = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center=True, \n",
        "samplewise_std_normalization=True, \n",
        "horizontal_flip = False, \n",
        "vertical_flip = False, \n",
        "height_shift_range = 0.1, \n",
        "width_shift_range = 0.1, \n",
        "rotation_range = 10, \n",
        "shear_range = 0.05,\n",
        "fill_mode = 'reflect', #'nearest',\n",
        "zoom_range= 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRzUGueyTfq7"
      },
      "source": [
        "The above would apply only to one set of images, whereas we want to apply the same set of image transformations on both our images and the masks. A convenient way to do so is to create a dictionary of desired augmentations and pass to separate generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxwY49TBTfq7"
      },
      "source": [
        "# we create two instances with the same arguments\n",
        "data_gen_args = dict(featurewise_center=True,\n",
        "                     featurewise_std_normalization=True,\n",
        "                     rotation_range=90,\n",
        "                     width_shift_range=0.1,\n",
        "                     height_shift_range=0.1,\n",
        "                     zoom_range=0.2)\n",
        "image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)\n",
        "mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**data_gen_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VKJVBUQTfrA"
      },
      "source": [
        "#### Creating and merging image and mask generator functions\n",
        "\n",
        "In order to use an image generator you need to specify how information flows through it, or where the data is read from. We want to read imagery from directories of files, so the appropriate ```flow``` type to use is ```flow_from_directory```.\n",
        "\n",
        "Below is a fully worked example that creates generators that randomly apply up to 90 degree rotations and horizontal flips. The ```flow_from_directory``` for each specifies no random shuffling, which is important to ensure that images and labels correspond with one another. The images are also read in as 512 x 512 pixels, and the batch size is the entire data set (i.e. ```len(all_images)```)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItQ_3zgLTfrB"
      },
      "source": [
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization=True,\n",
        "        shear_range=0,\n",
        "        zoom_range=0,\n",
        "        rotation_range=90,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "img_generator = train_datagen.flow_from_directory(\n",
        "        's2cloudless_imagery',\n",
        "        target_size=(512, 512),\n",
        "        batch_size=len(images),\n",
        "        class_mode=None, seed=111, shuffle=False)\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization=True,    \n",
        "        shear_range=0,\n",
        "        zoom_range=0,\n",
        "        rotation_range=90,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "mask_generator = test_datagen.flow_from_directory(\n",
        "        's2cloudless_label_imagery',\n",
        "        target_size=(512, 512),\n",
        "        batch_size=len(images),\n",
        "        class_mode=None, seed=111, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ckzP0IRTfrE"
      },
      "source": [
        "The following merges the two generators (and their flows) together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzuO4fvnTfrH"
      },
      "source": [
        "train_generator = (pair for pair in zip(img_generator, mask_generator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvi28loTfrL"
      },
      "source": [
        "#### Create and visualize arbitrary transformations to input imagery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjihXLHRTfrN"
      },
      "source": [
        "To access the ```next``` pair of image and label from the generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxewSHPbTfrO"
      },
      "source": [
        "x, y = next(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1ybR96OdD32"
      },
      "source": [
        "Take a look at the size and length of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVbFL1emV7tm"
      },
      "source": [
        "np.shape(np.max(y[10], axis=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n4G-j0UTfrR"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.imshow((x[10]).astype('uint8'), cmap='gray') #show the 10th image\n",
        "plt.imshow(np.max(y[10], axis=2)/255, cmap='gray', alpha=0.5) #show the 10th label as an alpha mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1CunymhTfrT"
      },
      "source": [
        "A little more visually appealing is a figure with alpha channels appropriately set, like we did above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrj6jCivTfrU"
      },
      "source": [
        "def get_pair(i):\n",
        "    img = x[i].astype('uint8')/255\n",
        "    msk = np.max(y[i], axis=2)/255\n",
        "    msk[msk>=.5]  = 1\n",
        "    msk[msk<.5] = 0\n",
        "    msk = np.stack((msk,)*3, axis=-1)\n",
        "    return np.concatenate([img, msk], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQn8SnWeTfrW"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(0))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(9))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(19))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(29))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(get_pair(39))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDFKfmz2_x-N"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>A brief introduction to <br/> Convolutional Neural Networks</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "    Here we explore in a little more detail convolutional neural networks, also known as <b>CNNs</b>, which are a specific type of neural networks used extensive in deep learning classification (and regression) tasks\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9h4rVH5_tyn"
      },
      "source": [
        "##### Basic principles\n",
        "\n",
        "* Convolutional Neural Networks (CNNs) are very similar to ordinary Neural Networks: they are made up of neurons that have learnable weights and biases.\n",
        "\n",
        "* CNN architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture.\n",
        "\n",
        "Natural images exhibit ”‘stationarity”’, meaning that the statistics of one part of the image are the same as any other part. This suggests that the features that we learn at one part of the image can also be applied to other parts of the image, and we can use the same features at all locations.\n",
        "\n",
        "More precisely, having learned features over small (say 8x8) patches sampled randomly from the larger image, we can then apply this learned 8x8 feature detector anywhere in the image. Specifically, we can take the learned 8x8 features and ”‘convolve”’ them with the larger image, thus obtaining a different feature activation value at each location in the image.\n",
        "\n",
        "#### Layers\n",
        "\n",
        "The layers consist of hierarchical filters that are designed to extract features of increasingly complexity\n",
        "\n",
        "* The input of each layer is the output of the previous one\n",
        "\n",
        "* The layer does not need to learn the whole concept at once, but actually build a chain of features that build that knowledge.\n",
        "\n",
        "* It learns the best way to map inputs to outputs (you don’t need to)\n",
        "\n",
        "\n",
        "![](https://i1.wp.com/www.michaelchimenti.com/wp-content/uploads/2017/11/Deep-Neural-Network-What-is-Deep-Learning-Edureka.png)\n",
        "\n",
        "To demonstrate some principles we will load an image, convert it to a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fONwL4vuTfrg"
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import rasterio, os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBZ3pBoJTfrn"
      },
      "source": [
        "with rasterio.open('s2cloudless_imagery'+os.sep+'data'+os.sep+'walker_s2cloudless-2016.jpg') as dataset:\n",
        "    image = np.array(dataset.read().T)\n",
        "\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEcZpjvKYAjM"
      },
      "source": [
        "image = tf.image.rgb_to_grayscale(tf.image.resize(image, (128,128)) )   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ts7No91Tfrr"
      },
      "source": [
        "To use this image with TensorFlow, we convert it into a `Tensor`. Many Tensorflow operations we will use in this project are meant for batches of images, i.e., of shape $(N, H, W, C)$ where $N$ is the number of images, $H$ and $W$ are height and width, and $C$ is the number of channels (3 = RGB, 1 = grayscale)\n",
        "\n",
        "Exapnd the dimensions of our image, and turn it into a batch consisting of just one image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_hWCRk8Tfrt"
      },
      "source": [
        "image = tf.convert_to_tensor(image, tf.float32, name=\"input_image\")\n",
        "image = tf.expand_dims(image, axis=0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uddGxfjnTfrz"
      },
      "source": [
        "print(\"Tensor shape: {}\".format(image.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgFQgLLZTfr3"
      },
      "source": [
        "#### Types of layers\n",
        "\n",
        "CNNs are generally composed of the following layers:\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/architecture-cnn.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
        "\n",
        "##### Convolution layer (CONV)\n",
        "\n",
        "[source](http://deeplearning.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/)\n",
        "\n",
        "The convolution layer (CONV) uses filters that perform convolution operations as it is scanning the input with respect to its dimensions. Its hyperparameters include the filter size and stride length. The resulting output is called feature map or activation map.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/convolution-layer-a.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzGbhIYq-OWb"
      },
      "source": [
        "# define a Guassian blur filter\n",
        "a = np.zeros([3, 3, 1, 1])\n",
        "a[1, 1, :, :] = 0.25\n",
        "a[0, 1, :, :] = 0.125\n",
        "a[1, 0, :, :] = 0.125\n",
        "a[2, 1, :, :] = 0.125\n",
        "a[1, 2, :, :] = 0.125\n",
        "a[0, 0, :, :] = 0.0625\n",
        "a[0, 2, :, :] = 0.0625\n",
        "a[2, 0, :, :] = 0.0625\n",
        "a[2, 2, :, :] = 0.0625"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABd-JCHOItVh"
      },
      "source": [
        "To pass the tensor to the convolution algorithm we can actually convert $(N, H, W, C)$ to $(N, C, H, W)$ then provide the argument ```data_format = 'NCHW'```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEA_Rn4ZApsP"
      },
      "source": [
        "# reshape data \n",
        "proj = np.array(image).reshape(1,1,128,128)\n",
        "\n",
        "# convolve with filter\n",
        "proj = tf.nn.conv2d(\n",
        "    proj, a,\n",
        "    strides = [ 1, 1, 1, 1 ],\n",
        "    padding = 'SAME',\n",
        "    data_format = 'NCHW'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2U2XMfRBt0q"
      },
      "source": [
        "# display convolved image\n",
        "plt.imshow(np.squeeze(proj), cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2cmyxRu1aqq"
      },
      "source": [
        "The `tf.nn.conv2d` layer is the backend to the keras `Conv2D` layer that we'll make repeated use of in this project, so it's worth spending some time getting to know its arguments:\n",
        "\n",
        "* `strides` is a list of 4 integers that defines the stride to take (how many pixels to shift in each of the 4 dimensions) for the kernel during the convolution operation\n",
        "\n",
        "* `padding` This is the instruction of what the convolution algorithm should do at the boundary of the data. It can be a specific padding dimension in pixels but more typically `SAME` and `VALID` are used. `VALID` means no padding (the filters slide only the _valid_ pixels not too near the boundary) and `SAME` means it will enforce the outputs to be the same as the inputs by temporarily setting the stride length to fit evenly at the boundary\n",
        "\n",
        "Convolutions like this are at the center of deep learning and convolutional neural networks. Their repeated application, with pooling layers, distill the relevant information in the image for the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg3fsWwPTfsR"
      },
      "source": [
        "##### Pooling (POOL)\n",
        "\n",
        "The pooling layer (POOL) is a downsampling operation, typically applied after a convolution layer, which does some spatial invariance. In particular, max and average pooling are special kinds of pooling where the maximum and average value is taken, respectively.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/max-pooling-a.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
        "\n",
        "For ***max-pooling*** and ***average-pooling***, the values in each window are aggregated into a single output, applying respectively the max or averaging operation. \n",
        "\n",
        "Apply the average pooling operator with stride lengths of 1, 1, 2, and 2 for, respectively, the $(N, C, H, W)$ dimensions, meaning the pooled image will be half the size in $H$ and $W$\n",
        "\n",
        "The code below uses a kernel size of 16 pixels in the $H$ dimension, which will result in vertical smearing of the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58SCKrc9adr2"
      },
      "source": [
        "avg_pooled_image = tf.nn.avg_pool(proj, ksize=[1, 1, 16, 1], strides=[1, 1, 1, 2], padding=\"SAME\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg3ckg4WK8z7"
      },
      "source": [
        "avg_res = avg_pooled_image.numpy() #eager execution allows direct conversion to numpy array\n",
        "np.shape(avg_res) # get shape of the array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o2h-ByuJygE"
      },
      "source": [
        "We used a kernel size of 16 pixels in the $H$ dimension, therefore the result is a pooled image that is smoothed vertically but not horizontally "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RLY6-reTfsR"
      },
      "source": [
        "plt.imshow(np.squeeze(avg_res), cmap=plt.cm.gray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHq3XHexTfsf"
      },
      "source": [
        "##### Fully Connected (FC)\n",
        "\n",
        "The fully connected layer (FC) operates on a flattened input where each input is connected to all neurons. If present, FC layers are usually found towards the end of CNN architectures and can be used to convert multidimensional features into a 1d vector of class scores.\n",
        "\n",
        "![](https://stanford.edu/~shervine/images/fully-connected.png)\n",
        "[source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCkWPrVpTfsk"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "\n",
        "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
        "    </small><br/>Introduction to <br/> Auto-encoder networks</h1>\n",
        "<br/>\n",
        "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
        "Autoencoders receive input data (image), encode it, then returns the same data on the output layer. They are kind of neural network that take an image and compress the information in it to a linear feature representation (vector) or \"code\". The \"code\" is upsampled by the decoder portion of the auto-encoder, called the \"decryption\" layer. The architecture preserves the dimensionality of input->output. But, the linear compression of the input leads to a bottleneck that does not transmit all features.  \n",
        "\n",
        "Below we will make a deep autoencoder that creates outputs from extremely compressed inputs, which could be used for image denoising.\n",
        "  \n",
        "</p>\n",
        "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
        "We can have more than one hidden layers and than we have so-called Deep Autoencoders. The U-Net and FCN models we will use later are two different types of deep autoencoders modified for for semantic segmentation.\n",
        "</p>\n",
        "        </tr>\n",
        "        </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKwk6UakAKWD"
      },
      "source": [
        "##### Preparing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87QFfO7s_TT_"
      },
      "source": [
        "Read each image and keep track of the minimum dimension of all images (later all images will be cropped to this square dimension)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft7gyhGHchfe"
      },
      "source": [
        "import rasterio, json, os\n",
        "import numpy as np\n",
        "\n",
        "# load the labels than contain the image list\n",
        "data = json.load(open('all_labels.json'))\n",
        "images = sorted(data.keys())\n",
        "\n",
        "# pre-allocate arrays for images and minimum dimensions\n",
        "all_images = []; N = []\n",
        "for image in images: # loop through each image file\n",
        "    with rasterio.open('s2cloudless_imagery'+os.sep+'data'+os.sep+image) as dataset:\n",
        "        tmp = np.squeeze(np.array(dataset.read().T)) #read the data and transpose\n",
        "        nx, ny, _ = np.shape(tmp) # get the dimensions\n",
        "        n = np.minimum(nx,ny) # get the minimum dimension\n",
        "        all_images.append(tmp) # append the image\n",
        "        N.append(n) #append the min dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaIj_ftR_OSn"
      },
      "source": [
        "Crop all images to the same minimum square extent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGkU5XhQhelC"
      },
      "source": [
        "n = np.min(N) # get the global minimum dimension\n",
        "all_images = [l[:n,:n] for l in all_images] # crop all images to that square dimension\n",
        "all_images = np.array(all_images) # convert list to a numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4_ZNzHJ-5ED"
      },
      "source": [
        "Check the shapes ($N$, $H$, $W$, $C$):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynBc5uHigYdk"
      },
      "source": [
        "print(all_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk96ZY4u-_yt"
      },
      "source": [
        "Define an output image size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrZ4qwE48ZKW"
      },
      "source": [
        "IMG_SIZE = 124"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf0YUS6N_B-0"
      },
      "source": [
        "Resize each image and keep only the first (red) band"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NLu4Ek_svml"
      },
      "source": [
        "#import tensorflow as tf\n",
        "all_images2 = np.zeros((40,IMG_SIZE,IMG_SIZE))\n",
        "counter = 0\n",
        "for k in all_images:\n",
        "    all_images2[counter,:,:] = np.squeeze(tf.image.resize(k, (IMG_SIZE,IMG_SIZE), method='nearest'))[:,:,0]\n",
        "    counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np9gkESs-zJR"
      },
      "source": [
        "Let's take a look at an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV9d71nPtPJP"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(all_images2[10].astype('uint8'), cmap='gray')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcU6WpML-Z4I"
      },
      "source": [
        "Use the first 30 images for training and the last 10 for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGOV6MF7eKAE"
      },
      "source": [
        "x_train = all_images2[:30,:,:]\n",
        "x_test = all_images2[30:,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxYj8AkX-RaG"
      },
      "source": [
        "Check the shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJccy3ERfKkv"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLzPheAL0O-"
      },
      "source": [
        "We no longer need ```all_images```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANcuWFuOzjOD"
      },
      "source": [
        "del all_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u8zUbAvApi-"
      },
      "source": [
        "Import the necessary layers and ```Model``` structure from keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUb1XOoL62_Y"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7V62g7-Tfso"
      },
      "source": [
        "##### Theory in action: Auto-encoding\n",
        "\n",
        "The following class carries out training and prediction for a deep convolutional autoencoder. The 'encoding' part uses 3 blocks of convolutional filters and 2 max pooling layers for 'auto'mated feature extraction. The feature vector, or 'code', is then upscaled using the reverse sequence of operations in the encoding part. Training is then used to optimize the weights of all the convolutional filters.\n",
        "\n",
        "This is exactly how we will later use deep autoencoder models for semantic segmentation. This exercise is designed to introduce you to the basic building blocks and concepts of the encoding-decoding concept.\n",
        "\n",
        "The goal in the following lines of code is to properly encode then decode an image wihout impacting the quality, despite the so-called _bottleneck_ which is the 'code' or low-dimesional feature representation. The bottleneck forces the model to learn a properly compressed representation of the salient image features extracted by the network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo1SuUkH0bgh"
      },
      "source": [
        "# make a python class \n",
        "class Deep_Autoencoder():\n",
        "    \n",
        "    def __init__(self):    \n",
        "        \n",
        "        ## the Encoding part\n",
        "        # use grayscale only so W x H x 1\n",
        "        in_ = Input(shape=(IMG_SIZE, IMG_SIZE, 1))\n",
        "        # 2d convolution layer with 32 output filters in the convolution, \n",
        "        # 3x3 pixel kernel, and Relu activation\n",
        "        _ = Conv2D(32, (3, 3), activation='relu', padding='same')(in_)\n",
        "        #'_' is being used as a intermediate variable that is overwritten in each layer\n",
        "        _ = MaxPooling2D((2, 2), padding='same')(_)\n",
        "        # a smaller convolutional block with 16 filters\n",
        "        _ = Conv2D(16, (3, 3), activation='relu', padding='same')(_)\n",
        "        # max pooling layer\n",
        "        _ = MaxPooling2D((2, 2), padding='same')(_)\n",
        "        # a third convolutional block\n",
        "        _ = Conv2D(16, (3, 3), activation='relu', padding='same')(_)\n",
        "        \n",
        "        ## The \"Code\" is the output of the 2nd max pooling\n",
        "        code = MaxPooling2D((2, 2), padding='same')(_)\n",
        "        \n",
        "        ## The Decoding part is the inverse operation to the above\n",
        "        _ = Conv2D(16, (3, 3), activation='relu', padding='same')(code)\n",
        "        _ = UpSampling2D((2, 2))(_)\n",
        "        _ = Conv2D(16, (3, 3), activation='relu', padding='same')(_)\n",
        "        _ = UpSampling2D((2, 2))(_)\n",
        "        _ = Conv2D(32, (3, 3), activation='relu')(_)\n",
        "        _ = UpSampling2D((2, 2))(_)\n",
        "        out_ = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(_)\n",
        "        \n",
        "        # set up model inputs and outputs, solver (RMSprop) and loss function\n",
        "        self._model = Model(in_, out_)\n",
        "        self._model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
        "        \n",
        "    # train is a callable function of the Deep_Autoencoder class object   \n",
        "    # for training the model\n",
        "    def do_training(self, in_train, in_test, batch_size, num_epochs):    \n",
        "        self._model.fit(in_train, # the train and validation data are the same in this case\n",
        "                        in_train,\n",
        "                        verbose = 1,\n",
        "                        epochs = num_epochs,\n",
        "                        batch_size = batch_size,\n",
        "                        shuffle = True,\n",
        "                        validation_data = (in_test, in_test))\n",
        "        \n",
        "    # returns a model prediction (a decoded image)\n",
        "    def get_image(self, encoded_images):\n",
        "        decoded_images = self._model.predict(encoded_images)\n",
        "        return decoded_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bviknth-0GXV"
      },
      "source": [
        "We brushed over an important line above that warrants some further exploration:\n",
        "\n",
        "`self._model.compile(optimizer='rmsprop', loss='binary_crossentropy')`\n",
        "\n",
        "The `compile` command is required before training the model. The `optimizer` will perform the gradient descent, and `loss` is the metric to optimize\n",
        "\n",
        "In the `fit` arguments, by setting `verbose` to 1 gives you a progress bar as the model trains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QosBOXTjcaqz"
      },
      "source": [
        "Prepare the inputs by scaling between 0 and 1 as floats, and reshaped as tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJVoLLCk1-xc"
      },
      "source": [
        "x_train = x_train.astype('float32') / 255. #make into a float between 0 and 1\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), IMG_SIZE, IMG_SIZE, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), IMG_SIZE, IMG_SIZE, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlCkqhyoatjB"
      },
      "source": [
        "Create an empty deep autoencoder class, then train for 100 iterations using a batch size of 8 images. Then, recover the decoded images using the trained model ```dae```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WKbVh7q6t_W"
      },
      "source": [
        "dae = Deep_Autoencoder()\n",
        "dae.do_training(x_train, x_test, 8, 100)\n",
        "decoded_imgs = dae.get_image(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnRujACOc12q"
      },
      "source": [
        "Visual comparison of inputs and autoencoded-decoded outputs. As you can see, the model does a reasonably good job at image mimickry "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0wXO-171q-N"
      },
      "source": [
        "#create a wide (16) and short (3) figure to show 10 images\n",
        "# and their autoencoded representations stacked top to bottom\n",
        "plt.figure(figsize=(16, 3))\n",
        "# loop through 10 sample images\n",
        "for i in range(10):\n",
        "    # inputs\n",
        "    subplot = plt.subplot(2, 10, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(IMG_SIZE, IMG_SIZE))\n",
        "    plt.gray()\n",
        "    subplot.get_xaxis().set_visible(False) #turn x-axis off\n",
        "    subplot.get_yaxis().set_visible(False) #turn y-axis off\n",
        "\n",
        "    # reconstructed inputs\n",
        "    subplot = plt.subplot(2, 10, i + 11)\n",
        "    plt.imshow(decoded_imgs[i].reshape(IMG_SIZE, IMG_SIZE))\n",
        "    plt.gray()\n",
        "    subplot.get_xaxis().set_visible(False)\n",
        "    subplot.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0JvxnvyfODY"
      },
      "source": [
        "> Going Further\n",
        "\n",
        "Do you think the semantic segmentation task of detecing water pixels would be easier or harder with the coarser detail autoencoder estimates?\n",
        "\n",
        "On the one hand, it might be easier because there is less extraneous detail preserved from the terrain surrounding the lakes, therefore it might be easier for a model to identify the wet pixels and more reliably distinguish them from the dry pixels.\n",
        "\n",
        "On the other hand, the loss of detail may also make the lake boundary fuzzy and harder to detect, thereby potentially introducing error in the metrics used to quantify water availability over time. You now have the tools to explore these issues further, by generating datasets based on images in their native resolution and format, versus filtered versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1IRSx0LcB0d"
      },
      "source": [
        "#### The U-Net model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cds0pFD3wkHg"
      },
      "source": [
        "Introduced in 2015, the U-Net model is still popular and is also commonly seen embedded in more complex deep learning models\n",
        "\n",
        "The U-Net model is a simple fully  convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. Mainly, it consists of two parts. \n",
        "\n",
        "*   Encoder: we apply a series of conv layers and downsampling layers  (max-pooling) layers to reduce the spatial size \n",
        "*   Decoder: we apply a series of upsampling layers to reconstruct the spatial size of the input. \n",
        "\n",
        "The two parts are connected using a concatenation layers among different levels. This allows learning different features at different levels. At the end we have a simple conv 1x1 layer to reduce the number of channels to 1. \n",
        "\n",
        "U-Net is symmetrical (hence the \"U\" in the name) and uses concatenation instead of addition to merge feature maps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXaAcBS4e75C"
      },
      "source": [
        "#### IoU metric\n",
        "\n",
        "The intersection over union (IoU) or Jacard metric is a simple metric used to evaluate the performance of a segmentation algorithm. Given two masks $y_{true}, y_{pred}$ we evaluate \n",
        "\n",
        "$$IoU = \\frac{y_{true} \\cap y_{pred}}{y_{true} \\cup y_{pred}}$$\n",
        "\n",
        "The numerator is the number of common pixels, and the denominator is the total number of pixels in the two sets together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVp-75iDe6ON"
      },
      "source": [
        "def mean_iou(y_true, y_pred):\n",
        "    yt0 = y_true[:,:,:,0] #use the 3d image, not the 4d tensor\n",
        "    #binarize and make a float\n",
        "    yp0 = tf.keras.backend.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
        "    # get the intersection (numerator in above equation) \n",
        "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
        "    # get the union (denominator)\n",
        "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
        "    # compute iou as the ratio unless the denominator is zero\n",
        "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kYoIiVUfRpc"
      },
      "source": [
        "# we'll need a few more layers from keras\n",
        "from tensorflow.keras.layers import Concatenate, Conv2DTranspose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7LgXRW7MP1L"
      },
      "source": [
        "##### Encoder\n",
        "\n",
        "Downsamples the 512  x 512 x 3 image progressively using six banks of convolutional filters, each using filters double in size to the previous, thereby progressively downsampling the inputs as features are extracted through max pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE0MU0FoMJ5_"
      },
      "source": [
        "inputs = Input((512, 512, 3))\n",
        "_tensor = inputs\n",
        "  \n",
        "#down sampling \n",
        "f = 8 #initially, use an 8-pixel kernel for the convolutional filter\n",
        "layers = []\n",
        "\n",
        "#cycle through 6 iterations, each time reusing '_tensor' \n",
        "#on each iteration ...\n",
        "#pass through 2 convolutional blocks, append to the 'layers' output list\n",
        "#then apply max pooling, and double the filter size for the next iteration\n",
        "for i in range(0, 6):\n",
        "   _tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)\n",
        "   _tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)\n",
        "   layers.append(_tensor)\n",
        "   _tensor = MaxPooling2D() (_tensor)\n",
        "   f = f*2\n",
        "   print(_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfkYitTbe_HT"
      },
      "source": [
        "A 'bottleneck' is just machine learning jargon for a very low-dimensional feature representation of a high dimensional input. Or, a relatively small vector of numbers that distill the essential information about a large image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YHo_-MDNdmj"
      },
      "source": [
        "ff2 = 64 ##use an 64-pixel kernel for the convolutional filter\n",
        "  \n",
        "#bottleneck \n",
        "j = len(layers) - 1\n",
        "_tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)\n",
        "_tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)\n",
        "_tensor = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_tensor)\n",
        "# use concatenate to merge feature maps\n",
        "_tensor = Concatenate(axis=3)([_tensor, layers[j]])\n",
        "j = j -1 \n",
        "\n",
        "print(_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699g8JleP5jh"
      },
      "source": [
        "An input of 512 x 512 x 3 (>780,000 numbers) has been distilled to a 'bottleneck' of 16 x 16 x 320 (<82,000 numbers) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i0M4SjdMX38"
      },
      "source": [
        "##### Decoder\n",
        "\n",
        "Upsamples the bottleneck into a 512  x 512 x 1 label image progressively using six banks of convolutional filters, each using filters half in size to the previous, thereby progressively upsampling the inputs as features are extracted through transpose convolutions and concatenation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifC_qlr0MT2x"
      },
      "source": [
        "#upsampling \n",
        "for i in range(0, 5):\n",
        "  ff2 = ff2//2\n",
        "  f = f // 2 \n",
        "  _tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)\n",
        "  _tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)\n",
        "  _tensor = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (_tensor)\n",
        "  _tensor = Concatenate(axis=3)([_tensor, layers[j]])\n",
        "  j = j -1 \n",
        "\n",
        "_tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)\n",
        "_tensor = Conv2D(f, 3, activation='relu', padding='same') (_tensor)  \n",
        "_tensor.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmrmKQ2V6qeY"
      },
      "source": [
        "Transposed convolution?\n",
        "\n",
        "This relatively new type of deep learning model layer convolves a dilated version of the input tensor, in order to upscale the output. The dilation operation consists of interleaving zeroed rows and columns between each pair of adjacent rows and columns in the input tensor. The dilation rate is the stride length (we use 2 x 2 above) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYu2SniNPUXU"
      },
      "source": [
        "Finally, make the classification layer using one final convolutional layers that essentially just maps (by squishing over 16 layers) the output of the previous layer to a single 2D output (with values ranging from 0 to 1) based on a sigmoid activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaG9CPBCPUiU"
      },
      "source": [
        "#classification layer\n",
        "outputs = Conv2D(1, 1, activation='sigmoid') (_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwWjQXJ7wlxm"
      },
      "source": [
        "Make a ```unet``` instance and print a summary to screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mQNZinGMegm"
      },
      "source": [
        "  #model creation \n",
        "  model = Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxVdwBCcd-Ws"
      },
      "source": [
        "It is a fairly large model with over 6 million parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JjjeUc2TftI"
      },
      "source": [
        "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
        "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
        "    <strong style=\"color: #ff5733\">Deliverable</strong>  \n",
        "    <br/>The deliverable for Part 3 is a jupyter notebook showing a workflow to set up U-Net models for training using NWPU-RESISC45 lake images and corresponding labels. This will mostly test your understanding of the generic workflow of setting up multiple models for sequential training, in order to evaluate and compare model outputs and ultimately decide which model is best for the task, as well as how to implement custom conditional random fields for refining labels and segmentations in Parts 4 and 5.\n",
        "    </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb1yZFk61Khy"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7r5Gz2ITftI"
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.layers import Concatenate, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from random import shuffle\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import json, os, glob\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZOXrnIavYKp"
      },
      "source": [
        "tf.test.is_gpu_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pswjETO3BC6"
      },
      "source": [
        "#ls\n",
        "#%%file -b \"NWPU_images.zip\"\n",
        "#%find . -type f -ls -exec file -b {} \\;\n",
        "# this uploads, but is no better than uploading from gdrive directly. Still can't unzip.\n",
        "#!wget \"https://drivh.google.com/file/d/1MHZhTl1GgCTX_Q0Dv-iBzexQEKq7AHyA/view?usp=sharing\" -O \"NWPU_images.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7yPLA1D5VaE"
      },
      "source": [
        "#%rm \"NWPU_images (1).zip\"\n",
        "#%rm \"all_labels (1).json\"\n",
        "#%mv \"NWPU_images.zip\" \"NWPU_images_orig.zip\"\n",
        "#%mv \"NWPU_images (1).zip\" \"NWPU_images_second.zip\"\n",
        "#!file -b \"NWPU_images.zip\"\n",
        "#!7z x \"NWPU_images.zip\"\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EyTdHjSva0e"
      },
      "source": [
        "#https://drive.google.com/file/d/1MHZhTl1GgCTX_Q0Dv-iBzexQEKq7AHyA/view?usp=sharing\n",
        "if colab == 1:\n",
        "  file_id = '14kkcuU6wd9UMvjaDrg3PNI-e_voCi8HL'\n",
        "  destination = 'NWPU_images.zip'\n",
        "  download_file_from_google_drive(file_id, destination)\n",
        "#  nwpu_images = files.upload()\n",
        "#  unzip(nwpu_images)\n",
        "#  file_id = '1MHZhTl1GgCTX_Q0Dv-iBzexQEKq7AHyA'\n",
        "#  destination = 'NWPU_images.zip'\n",
        "#  download_file_from_google_drive(file_id, destination,False)\n",
        "  #colab : unzip(\"NWPU_images.zip\")\n",
        "#  with zipfile.ZipFile(f, 'r') as zip_ref:\n",
        "#    zip_ref.extractall()\n",
        "\n",
        "#  unzip('NWPU_images.zip')\n",
        "#else:\n",
        "#  unzip('NWPU_images.zip')   \n",
        "unzip('NWPU_images.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}