{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGhkrMQX9KPn"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<tr style=\"vertical-align: top; padding: 0; margin: 0;background-color: #ffffff\">\n",
    "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
    "    <p style=\"background: #182AEB; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
    "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #ffffff;\">Deep Learning </span> for Satellite Image Classification (Manning Publications)</span><br/>by <em>Daniel Buscombe</em></strong><br/><br/>\n",
    "        <strong>> Chapter 5: Model Optimization </strong><br/>\n",
    "    </p>           \n",
    "        \n",
    "<p style=\"border: 1px solid #182AEB; border-left: 15px solid #182AEB; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #182AEB\">What you learned in Part 4.</strong>  \n",
    "    <br/>In Part 4, you trained U-Net models to segment water pixels in imagery\n",
    "    </p>\n",
    "    \n",
    "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #ff5733\">What you will learn in this Part.</strong>  \n",
    "    <br/>In Part 5, you will optimize the models you made in Part 4. You will first learn how to tune hyper-parameters, add regularization and modify model architectures in several ways, and deal with class imbalance. Then, you will learn how to use a machine learning model known as a ‘fully-connected conditional random field” to refine label images, so labels are as accurate as possible.\n",
    "    </p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROJhVuY4_VOB"
   },
   "source": [
    "#### Preliminaries for Colab\n",
    "\n",
    "Like Part 3 and 4 previously, below are some convenience functions for those working on Google Colab with a GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab = 0\n",
    "colab = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48789,
     "status": "ok",
     "timestamp": 1572819087100,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "I-lc86ey-HOb",
    "outputId": "d9e25c96-3e24-41e2-95f8-ced7856f1a46"
   },
   "outputs": [],
   "source": [
    "if colab==1:\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install --default-timeout=1000 tensorflow-gpu==2.0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8ruWT7u_ZLB"
   },
   "source": [
    "You may have to restart the runtime and/or change runtime type here, if the following doesn't show Tensorflow version 2, and a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1572819133184,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "v7gY9Wi2_aHF",
    "outputId": "3a739855-8e83-4a41-e37c-10bfac00f265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7i8OMteVsrG"
   },
   "source": [
    "Convenience functions if you need to download example (minimal) imagery sets derived from NWPU and Sentinel-2 cloudless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt7x8mRRVs7m"
   },
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "#s2 cloudless imagery\n",
    "file_id = '1iMfIjr_ul49Ghs2ewazjCt8HMPfhY47h'\n",
    "destination = 's2cloudless_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#s2 cloudless labels\n",
    "file_id = '1c7MpwKVejoUuW9F2UaF_vps8Vq2RZRfR'\n",
    "destination = 's2cloudless_label_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#nwpu imagery\n",
    "file_id = '1gtuqy1VlU8-M5IEMnmiSuTlI5PxQPnGB'\n",
    "destination = 'nwpu_images.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#nwpu labels\n",
    "file_id = '1W5LGbcYAcFbG5YjLgX_ekBn0u5Rno35x'\n",
    "destination = 'nwpu_label_images.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sommbr0GVy_F"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip(f):\n",
    "    \"\"\"\n",
    "    f = file to be unzipped\n",
    "    \"\"\"    \n",
    "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        \n",
    "if colab==1:\n",
    "    unzip('s2cloudless_imagery.zip')\n",
    "    unzip('s2cloudless_label_imagery.zip')   \n",
    "    unzip('nwpu_images.zip')\n",
    "    unzip('nwpu_label_images.zip')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa9K6-E2T4fR"
   },
   "source": [
    "Import the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cvYaMkSR-JKj"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import json, os\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYgpaO2O9KPr"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Training tuning strategies</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "We will set up a baseline model with no optimization using the NWPU imagery, then explore the following training optimization strategies:\n",
    "<ul>\n",
    "  <li>Building a bigger model with more layers</li>    \n",
    "  <li>Using Early Stopping and Adaptive Learning Rates</li> \n",
    "  <li>Using a bigger model (and dropout)</li> \n",
    "  <li>Using regularization (Batch Normalization)</li> \n",
    "  <li>Using residual connections</li> \n",
    "</ul>\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsLTcU6OJMAh"
   },
   "source": [
    "#### Getting things set up with a baseline model\n",
    "Zip through all these functions that we defined in the last Part. \n",
    "\n",
    "Like in the last Part, we will define the ```unet``` model, create model training callbacks, and generate augmented imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkJBCYCWM6qi"
   },
   "source": [
    "We use Keras callbacks to implement learning rate decay if the validation loss does not improve for 5 continues epochs. Called \"reduce loss on plateau\"\n",
    "\n",
    "Also, we implement early stopping if the validation loss does not improve for 5 continuous epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fo2ls1vA9KPy"
   },
   "outputs": [],
   "source": [
    "def image_batch_generator(files, batch_size = 32, sz = (512, 512)):\n",
    "  \n",
    "  while True: # this is here because it will be called repeatedly by the training function\n",
    "    \n",
    "    #extract a random subset of files of length \"batch_size\"\n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs (x) and outputs (y)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    #cycle through each image in the batch\n",
    "    for f in batch:\n",
    "\n",
    "        #preprocess the raw images \n",
    "        rawfile = f'nwpu_images/data/{f}'\n",
    "        raw = Image.open(rawfile)\n",
    "        raw = raw.resize(sz)\n",
    "        raw = np.array(raw)\n",
    "\n",
    "        #check the number of channels because some of the images are RGBA or GRAY\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.stack((raw,)*3, axis=-1)\n",
    "\n",
    "        else:\n",
    "            raw = raw[:,:,0:3]\n",
    "            \n",
    "        #get the image dimensions, find the min dimension, then square the image off    \n",
    "        nx, ny, nz = np.shape(raw)\n",
    "        n = np.minimum(nx,ny)\n",
    "        raw = raw[:n,:n,:] \n",
    "            \n",
    "        batch_x.append(raw)\n",
    "        \n",
    "        #get the masks. \n",
    "        maskfile = rawfile.replace('nwpu_images','nwpu_label_images')+'_mask.jpg'\n",
    "        mask = Image.open(maskfile)\n",
    "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
    "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
    "        # water pixels are always greater than 100\n",
    "        mask = (mask>200).astype('int')\n",
    "        \n",
    "        mask = mask[:n,:n]\n",
    "\n",
    "        batch_y.append(mask)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
    "    batch_y = np.array(batch_y)\n",
    "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
    "\n",
    "    yield (batch_x, batch_y) #yield both the image and the label together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WR_LULokT4ff"
   },
   "source": [
    "We've seen code like the below in the previous Part, setting up batch size, proportion of the dataset to train with, getting randomized lists of test and train file names, and finally setting up generators for model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3Fu3vdZ-nsN"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "prop_train = 0.6\n",
    "\n",
    "all_files = os.listdir('nwpu_images/data')\n",
    "shuffle(all_files)\n",
    "\n",
    "split = int(prop_train * len(all_files))\n",
    "\n",
    "#split into training and testing\n",
    "train_files = all_files[0:split]\n",
    "test_files  = all_files[split:]\n",
    "\n",
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2U2y96CiT4fi"
   },
   "source": [
    "A customary check that things worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1572819501376,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "8rIwVxdIIWEV",
    "outputId": "3e65d04f-eeef-4bf4-e0b9-690f7e88ff60"
   },
   "outputs": [],
   "source": [
    "x, y = next(train_generator)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(y[0].squeeze(), cmap='gray', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ajq5OJsUT4fo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKPDSbt3T4ft"
   },
   "outputs": [],
   "source": [
    "# a tolerance for the training.\n",
    "min_delta = 0.0001\n",
    "\n",
    "# minimum learning rate (lambda)\n",
    "min_lr = 0.0001\n",
    "\n",
    "# the factor applied to the learning rate when the appropriate triggers are made\n",
    "factor = 0.8\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "filepath = 'unet'+str(batch_size)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1572819509390,
     "user": {
      "displayName": "Daniel Buscombe",
      "photoUrl": "",
      "userId": "01832231008732716345"
     },
     "user_tz": 420
    },
    "id": "haW5J3F1-xax",
    "outputId": "5dd61647-fcd7-4fb9-f83c-84cc6a78834f"
   },
   "outputs": [],
   "source": [
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generator(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmoMd5t7T4gi"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: changing model architecture <br/>using residual connections</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "A standard approach is to pass the input image goes through multiple convolutions and obtain high-level features. In a network architecture with <em>residual layers</em> or connections, each layer gets to see both the output from the previous layer (standard) as well as the inputs to that layer. So it not only sees the ouputs but also the data used to learn that output\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "566oTTdYT4gj"
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1000/1*4wx7szWCBse9-7eemGQJSw.png)\n",
    "\n",
    "Import `Activation` and `Add` layers from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUPYE3s5T4gj"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation, Add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7B6b4HWT4gm"
   },
   "source": [
    "Create a new UNet model function. This time we'll use a few convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_act(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation(\"relu\")(x)\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = batchnorm_act(x)\n",
    "    return Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
    "\n",
    "def bottleneck_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "    \n",
    "    bottleneck = Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    bottleneck = batchnorm_act(bottleneck)\n",
    "    \n",
    "    return Add()([conv, bottleneck])\n",
    "\n",
    "def res_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
    "    \n",
    "    bottleneck = Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    bottleneck = batchnorm_act(bottleneck)\n",
    "    \n",
    "    return Add()([bottleneck, res])\n",
    "\n",
    "def upsamp_concat_block(x, xskip):\n",
    "    u = UpSampling2D((2, 2))(x)\n",
    "    return Concatenate()([u, xskip])\n",
    "\n",
    "def res_unet(sz, f):\n",
    "    inputs = Input(sz)\n",
    "    \n",
    "    ## downsample  \n",
    "    e1 = bottleneck_block(inputs, f); f = int(f*2)\n",
    "    e2 = res_block(e1, f, strides=2); f = int(f*2)\n",
    "    e3 = res_block(e2, f, strides=2); f = int(f*2)\n",
    "    e4 = res_block(e3, f, strides=2); f = int(f*2)\n",
    "    _ = res_block(e4, f, strides=2)\n",
    "    \n",
    "    ## bottleneck\n",
    "    b0 = conv_block(_, f, strides=1)\n",
    "    _ = conv_block(b0, f, strides=1)\n",
    "    \n",
    "    ## upsample\n",
    "    _ = upsamp_concat_block(_, e4)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e3)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e2)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e1)\n",
    "    _ = res_block(_, f)\n",
    "    \n",
    "    ## classify\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(_)\n",
    "    \n",
    "    #model creation \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdjw9DNG9KRE"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: Dealing with class imbalance <br/> using dice loss</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "    the network tends to <b>ignore smaller classes</b>. A soft dice loss could be used to train a model. Unlike the IoU metric, the numerator is the number of correctly classified pixels, and the denominator is the total number of pixels in a class that is in both estimated and ground truth. Thus it is insensitive to the number of pixels total in each class.  \n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zb_wsYWCT4g3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyYChIOdT4g5"
   },
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = tf.reshape(tf.dtypes.cast(y_true, tf.float32), [-1])\n",
    "    y_pred_f = tf.reshape(tf.dtypes.cast(y_pred, tf.float32), [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYBMlohv9KP2"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: Refining label images <br/> using conditional random fields</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "It is common to use some form of post-processing algorithm to refine either labels for training, or model predictions, on an image-by-image basis. A popular approach is called a fully connected or dense conditional random field or CRF. Each image is paired with a label image and the label image is refined based on the image. The CRF makes a model for the label given the image, then assesses the likelihood of each label given the CRF model. The model can be based on relative color differences between a label and the CRF model's idea of what color that label is (within a tolerance), and/or relative spatial differences (i.e. location within the image) between a given label and the CRF model's idea of the location of that label, again within a tolerance.\n",
    "</p>\n",
    "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
    "    We're using the CRF to correct a pixelwise (dense) label image. It can also be used as an 'inpainting' algorithm if only sparse areas of the image have labels. This was the approach proposed by <a href=\"https://www.mdpi.com/2076-3263/8/7/244\">Buscombe and Ritchie (2018)</a> \n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJQcdKlET4hQ"
   },
   "source": [
    "Uncomment below to install the module and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeQEUblY9KP3"
   },
   "outputs": [],
   "source": [
    "#!pip install cython\n",
    "#!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ebw5D7L9KP6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydensecrf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ef8330344b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydensecrf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdensecrf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydensecrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munary_from_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydensecrf'"
     ]
    }
   ],
   "source": [
    "from pydensecrf import densecrf\n",
    "from pydensecrf.utils import unary_from_labels\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will refine a label image based on an undelying image, using a dense CRF. The model's hyperparameters (i.e. specified by you, not by training) are:\n",
    " \n",
    "```compat_spat``` is a non-dimensional parameter that penalizes small pieces of segmentation that are spatially isolated -- it enforces more spatially consistent segmentations. Larger values means larger pieces of segmentation are allowed.\n",
    "\n",
    "```compat_col``` is a non-dimensional parameter that penalizes pieces of segmentation that are less uniform in color -- it enforces more consistent segmentations in colorspace. Larger values means pieces of segmentation with less similar image intesity are allowed.\n",
    "\n",
    "`theta_spat` and `theta_col` are tolerances in location and intensity, respectively. Larger values means pixel pairs can be considered to be the same class label with less similar location or intensity.\n",
    "\n",
    "`num_iter` is the number of iterations to run the CRF model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxLTPfE-9KP-"
   },
   "outputs": [],
   "source": [
    "def crf_labelrefine(input_image, predicted_labels):\n",
    "    \n",
    "    compat_spat=10\n",
    "    compat_col=30\n",
    "    theta_spat = 20\n",
    "    theta_col = 80\n",
    "    num_iter = 7\n",
    "    \n",
    "    h, w = input_image.shape[:2] #get image dimensions\n",
    "    \n",
    "    d = densecrf.DenseCRF2D(w, h, 2) #create a CRF object\n",
    "\n",
    "    # For the predictions, densecrf needs 'unary potentials' which are labels (water or no water)\n",
    "    predicted_unary = unary_from_labels(predicted_labels.astype('int')+1, num_classes, gt_prob= 0.51)\n",
    "    \n",
    "    # set the unary potentials to CRF object\n",
    "    d.setUnaryEnergy(predicted_unary)\n",
    "\n",
    "    # to add the color-independent term, where features are the locations only:\n",
    "    d.addPairwiseGaussian(sxy=(theta_spat, theta_spat), compat=compat_spat, kernel=densecrf.DIAG_KERNEL,\n",
    "                          normalization=densecrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    input_image_uint = (input_image * 255).astype(np.uint8) #enfore unsigned 8-bit\n",
    "    # to add the color-dependent term, i.e. 5-dimensional features are (x,y,r,g,b) based on the input image:    \n",
    "    d.addPairwiseBilateral(sxy=(theta_col, theta_col), srgb=(5, 5, 5), rgbim=input_image_uint,\n",
    "                           compat=compat_col, kernel=densecrf.DIAG_KERNEL, \n",
    "                           normalization=densecrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    # Finally, we run inference to obtain the refined predictions:\n",
    "    refined_predictions = np.array(d.inference(num_iter)).reshape(num_classes, h, w)\n",
    "    \n",
    "    # since refined_predictions will be a 2 x width x height array, \n",
    "    # each slice respresenting probability of each class (water and no water)\n",
    "    # therefore we return the argmax over the zeroth dimension to return a mask\n",
    "    return np.argmax(refined_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen this function before, but this time we'll need to add a line that carries out the CRF post-processing on the input (training) mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_batch_generator(files, batch_size = 32, sz = (512, 512)):\n",
    "  \n",
    "  while True: # this is here because it will be called repeatedly by the training function\n",
    "    \n",
    "    #extract a random subset of files of length \"batch_size\"\n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs (x) and outputs (y)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    #cycle through each image in the batch\n",
    "    for f in batch:\n",
    "\n",
    "        #preprocess the raw images \n",
    "        rawfile = f'nwpu_images/data/{f}'\n",
    "        raw = Image.open(rawfile)\n",
    "        raw = raw.resize(sz)\n",
    "        raw = np.array(raw)\n",
    "\n",
    "        #check the number of channels because some of the images are RGBA or GRAY\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.stack((raw,)*3, axis=-1)\n",
    "\n",
    "        else:\n",
    "            raw = raw[:,:,0:3]\n",
    "            \n",
    "        #get the image dimensions, find the min dimension, then square the image off    \n",
    "        nx, ny, nz = np.shape(raw)\n",
    "        n = np.minimum(nx,ny)\n",
    "        raw = raw[:n,:n,:] \n",
    "            \n",
    "        batch_x.append(raw)\n",
    "        \n",
    "        #get the masks. \n",
    "        maskfile = rawfile.replace('nwpu_images','nwpu_label_images')+'_mask.jpg'\n",
    "        mask = Image.open(maskfile)\n",
    "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
    "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
    "        # water pixels are always greater than 100\n",
    "        mask = (mask>200).astype('int')\n",
    "        \n",
    "        mask = mask[:n,:n]\n",
    "        \n",
    "        # use CRF to refine mask before it is used as a label\n",
    "        mask = crf_labelrefine(raw, mask).squeeze()\n",
    "\n",
    "        batch_y.append(mask)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
    "    batch_y = np.array(batch_y)\n",
    "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
    "\n",
    "    yield (batch_x, batch_y) #yield both the image and the label together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model like previously. This time both input and output masks of an image will be hopefully improved using the CRF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<h1 style=\"width: 100%; text-align: left; padding: 0px 25px;\"><small style=\"color: #182AEB;\">\n",
    "    </small><br/>Optimization strategy: <br/> using ensemble predictions</h1>\n",
    "<br/>\n",
    "<p style=\"border-left: 15px solid #182AEB; text-align:justify; padding: 0 10px;\">\n",
    "    It is common to train several models and average their predictions. This is called <b>ensemble</b> modeling because you are using the group of models to make a single prediction. \n",
    "</p>\n",
    "<p style=\"border-left: 15px solid #6019D6; padding: 0 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #6019D6;\">Tip.</strong> \n",
    "    We have several trained models. Here we will apply them all to the same imagery and average the masks, to see if that produces a more stable estimate\n",
    "</p>\n",
    "        </tr>\n",
    "        </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two models, compile them, and assign them the weights from two different 'h5' files saved during model training using the `load_weights` utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '1BEw63yYh1Wt6Dbz85gm-U1wmi2Nyddbo'\n",
    "destination = 'opt_model_5_res_dice'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#s2 cloudless labels\n",
    "file_id = '1QgJqz2el-9it4e9rAUUeQgWMpZbcMlKL'\n",
    "destination = 'opt_model_6_res_dice2'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "m1 = tf.keras.models.load_model('opt_model_6_res_dice2',\n",
    "                               custom_objects = {\"dice_coef_loss\":dice_coef_loss,\n",
    "                                                \"dice_coef\":dice_coef})\n",
    "m2 = tf.keras.models.load_model('opt_model_5_res_dice',\n",
    "                               custom_objects = {\"dice_coef_loss\":dice_coef_loss,\n",
    "                                                \"dice_coef\":dice_coef})\n",
    "#m1 = tf.keras.models.load_model('/Users/hankmobley/Downloads/opt_model_6_res_dice2',\n",
    "#                               custom_objects = {\"dice_coef_loss\":dice_coef_loss,\n",
    "#                                                \"dice_coef\":dice_coef})\n",
    "#m2 = tf.keras.models.load_model('/Users/hankmobley/Downloads/opt_model_5_res_dice',\n",
    "#                               custom_objects = {\"dice_coef_loss\":dice_coef_loss,\n",
    "#                                                \"dice_coef\":dice_coef})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab==1:\n",
    "    !mkdir weights\n",
    "    m1.save_weights('weights/res_dice_crf_unet2.h5')\n",
    "    m2.save_weights('weights/res_dice_unet2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = res_unet((512, 512, 3), 32)\n",
    "model1.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model1.load_weights('weights/res_dice_crf_unet2.h5')\n",
    "#model1 = tf.model1.load_model('/Users/hankmobley/Downloads/opt_model_6_res_dice2')\n",
    "\n",
    "model2 = res_unet((512, 512, 3), 32)\n",
    "model2.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model2.load_weights('weights/res_dice_unet2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for 1) getting estimates from models 1 and 2; 2) take the pixelwise maximum of the two; 3) obtain a CRF-refined estimated water mask, and 4) compute an IOU score evaluated against the real mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(x, y, model1, model2):\n",
    "    #predict the mask \n",
    "    pred1 = model1.predict(np.expand_dims(x, 0))\n",
    "    pred2 = model2.predict(np.expand_dims(x, 0))\n",
    "    \n",
    "    #mask post-processing \n",
    "    msk  = np.maximum(pred1.squeeze(), pred2.squeeze())\n",
    "    # binarize\n",
    "    msk[msk >= 0.5] = 1 \n",
    "    msk[msk < 0.5] = 0\n",
    "    \n",
    "    # use CRF to refine mask before it is used as a label\n",
    "    msk = crf_labelrefine(x.squeeze(), msk).squeeze()\n",
    "        \n",
    "    # return the prediction and the IOU score of the prediction\n",
    "    return msk, mean_iou(y, msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a batch of images and labels and run the `get_pred` command on the first pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'densecrf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-53fde180afc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#x, y = next(image_batch_generator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-68aa3d4ce1bb>\u001b[0m in \u001b[0;36mimage_batch_generator\u001b[0;34m(files, batch_size, sz)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# use CRF to refine mask before it is used as a label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf_labelrefine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mbatch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-466107d91dd1>\u001b[0m in \u001b[0;36mcrf_labelrefine\u001b[0;34m(input_image, predicted_labels)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#get image dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdensecrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseCRF2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#create a CRF object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# For the predictions, densecrf needs 'unary potentials' which are labels (water or no water)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'densecrf' is not defined"
     ]
    }
   ],
   "source": [
    "#x, y = next(image_batch_generator)\n",
    "x, y = next(test_generator)\n",
    "ypred = get_pred(x[0], y[0], model1, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the label mask and the ensemble estimate side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(y[0].squeeze(), cmap='gray', alpha=0.5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(ypred.squeeze(), cmap='gray', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7pff5C89KRM"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "<p style=\"border: 1px solid #ff5733; border-left: 15px solid #ff5733; padding: 10px; text-align:justify;\">\n",
    "    <strong style=\"color: #ff5733\">Deliverable</strong>  \n",
    "    <br/>The deliverable for Part 5 is a jupyter notebook showing a workflow to optimize the training of a model using the NWPU-RESISC45 lake images and corresponding labels, for the purposes of estimating lake area over time at sites represented in the Sentinel-2 imagery. The notebook will also show how to refine image labels based on a CRF model using both image color and relative spatial location in the image to make predictions, using optimized tunable parameters. This will mostly test your understanding of the generic yet complex workflow of optimizing model training, and applying a model trained on one dataset to another similar dataset, for the operational purpose of developing a time-series of lake areas at critical sites. \n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part5_Optimizing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
