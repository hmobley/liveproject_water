{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hmobley/liveproject_water/blob/master/solutions/milestone5/5_Model_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIkpoDrSyUOg"
   },
   "source": [
    "<table style=\"font-size: 1em; padding: 0; margin: 0;\">\n",
    "\n",
    "<tr style=\"vertical-align: top; padding: 0; margin: 0;background-color: #ffffff\">\n",
    "        <td style=\"vertical-align: top; padding: 0; margin: 0; padding-right: 15px;\">\n",
    "    <p style=\"background: #182AEB; color:#ffffff; text-align:justify; padding: 10px 25px;\">\n",
    "        <strong style=\"font-size: 1.0em;\"><span style=\"font-size: 1.2em;\"><span style=\"color: #ffffff;\">Deep Learning </span> for Satellite Image Classification</span> (Manning Publications)<br/>by <em>Daniel Buscombe</em></strong><br/><br/>\n",
    "        <strong>> Chapter 5: Deliverable Solution </strong><br/>\n",
    "    </p>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0u6x-FlyUOp"
   },
   "source": [
    "#### Preliminaries for Colab\n",
    "\n",
    "Like Part 3 and 4, below are some convenience functions for those working on Google Colab with a GPU runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2o_y1YvyUOy",
    "outputId": "e35c74e9-fb61-44d4-8ca4-477a3a88a19f"
   },
   "outputs": [],
   "source": [
    "colab = 0\n",
    "#colab = 1\n",
    "\n",
    "if colab==1:\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install --default-timeout=1000 tensorflow-gpu==2.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qXitQbvPzAnR"
   },
   "outputs": [],
   "source": [
    "load_res_dice_mix_history = 1\n",
    "if load_res_dice_mix_history:\n",
    "  import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cCSFK92Qyh4T"
   },
   "outputs": [],
   "source": [
    "# must be a pre-written method for this!\n",
    "def clear_drive():\n",
    "  if colab == 1:\n",
    "    !rm -r nwpu_images\n",
    "    !rm -r nwpu_label_images\n",
    "    !rm -r s2cloudless_imagery\n",
    "    !rm -r s2cloudless_label_imagery\n",
    "    !rm nwpu_images.zip\n",
    "    !rm nwpu_label_images.zip\n",
    "    !rm opt_model_1_early\n",
    "    !rm opt_model_1_early_hist\n",
    "    !rm opt_model_2_big\n",
    "    !rm opt_model_2_big_hist\n",
    "    !rm opt_model_3_bn\n",
    "    !rm opt_model_3_bn_hist\n",
    "    !rm opt_model_4_res\n",
    "    !rm opt_model_4_res_hist\n",
    "    !rm opt_model_5_res_dice\n",
    "    !rm opt_model_5_res_dice_hist\n",
    "    !rm opt_model_6_res_dice2\n",
    "    !rm opt_model_6_res_dice2_hist\n",
    "    !rm s2cloudless_imagery.zip\n",
    "    !rm s2cloudless_label_imagery.zip\n",
    "    !rm res_unet8.h5\n",
    "    !rm res_dice_unet2.h5\n",
    "    !rm -r weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZpK55RSy4n-",
    "outputId": "e5103596-fae9-4fbe-a2db-c97b7d51b8d3"
   },
   "outputs": [],
   "source": [
    "clear_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Adq0gEvuyUO2",
    "outputId": "0d92b3fe-76d0-46ba-c414-a73f89d93db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rEBeYB4yUO4"
   },
   "source": [
    "Convenience functions if you need to download example (minimal) imagery sets derived from NWPU and Sentinel-2 cloudless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s_AqPRPcyUPB"
   },
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n",
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "#s2 cloudless imagery\n",
    "file_id = '1iMfIjr_ul49Ghs2ewazjCt8HMPfhY47h'\n",
    "destination = 's2cloudless_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#s2 cloudless labels\n",
    "file_id = '1c7MpwKVejoUuW9F2UaF_vps8Vq2RZRfR'\n",
    "destination = 's2cloudless_label_imagery.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#nwpu imagery\n",
    "file_id = '1gtuqy1VlU8-M5IEMnmiSuTlI5PxQPnGB'\n",
    "destination = 'nwpu_images.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#nwpu labels\n",
    "file_id = '1W5LGbcYAcFbG5YjLgX_ekBn0u5Rno35x'\n",
    "destination = 'nwpu_label_images.zip'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jM-HWaNZyUPK"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip(f):\n",
    "    \"\"\"\n",
    "    f = file to be unzipped\n",
    "    \"\"\"    \n",
    "    with zipfile.ZipFile(f, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        \n",
    "if colab==1:\n",
    "    unzip('s2cloudless_imagery.zip')\n",
    "    unzip('s2cloudless_label_imagery.zip')   \n",
    "    unzip('nwpu_images.zip')\n",
    "    unzip('nwpu_label_images.zip')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt_OVZeYyUPM"
   },
   "source": [
    "#### Install libraries\n",
    "\n",
    "Uncomment below to install the module and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7mE7m_1KyUPN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /Users/hankmobley/opt/anaconda3/envs/liveproject/lib/python3.7/site-packages (0.29.21)\n",
      "Collecting git+https://github.com/lucasb-eyer/pydensecrf.git\n",
      "  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f\n",
      "Building wheels for collected packages: pydensecrf\n",
      "  Building wheel for pydensecrf (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/hankmobley/opt/anaconda3/envs/liveproject/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-wheel-on6hi335\n",
      "       cwd: /private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/\n",
      "  Complete output (21 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "  copying pydensecrf/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "  copying pydensecrf/utils.py -> build/lib.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "  running build_ext\n",
      "  building 'pydensecrf.eigen' extension\n",
      "  creating build/temp.macosx-10.7-x86_64-3.7\n",
      "  creating build/temp.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/hankmobley/opt/anaconda3/envs/liveproject/include -arch x86_64 -I/Users/hankmobley/opt/anaconda3/envs/liveproject/include -arch x86_64 -Ipydensecrf/densecrf/include -Ipydensecrf -I/Users/hankmobley/opt/anaconda3/envs/liveproject/include/python3.7m -c pydensecrf/eigen.cpp -o build/temp.macosx-10.7-x86_64-3.7/pydensecrf/eigen.o\n",
      "  warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "  In file included from pydensecrf/eigen.cpp:644:\n",
      "  In file included from pydensecrf/densecrf/include/Eigen/Dense:1:\n",
      "  pydensecrf/densecrf/include/Eigen/Core:22:10: fatal error: 'complex' file not found\n",
      "  #include <complex>\n",
      "           ^~~~~~~~~\n",
      "  1 warning and 1 error generated.\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for pydensecrf\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pydensecrf\n",
      "Failed to build pydensecrf\n",
      "Installing collected packages: pydensecrf\n",
      "    Running setup.py install for pydensecrf ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/hankmobley/opt/anaconda3/envs/liveproject/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-record-8xog2kz4/install-record.txt --single-version-externally-managed --compile --install-headers /Users/hankmobley/opt/anaconda3/envs/liveproject/include/python3.7m/pydensecrf\n",
      "         cwd: /private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/\n",
      "    Complete output (21 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-10.7-x86_64-3.7\n",
      "    creating build/lib.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "    copying pydensecrf/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "    copying pydensecrf/utils.py -> build/lib.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "    running build_ext\n",
      "    building 'pydensecrf.eigen' extension\n",
      "    creating build/temp.macosx-10.7-x86_64-3.7\n",
      "    creating build/temp.macosx-10.7-x86_64-3.7/pydensecrf\n",
      "    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/hankmobley/opt/anaconda3/envs/liveproject/include -arch x86_64 -I/Users/hankmobley/opt/anaconda3/envs/liveproject/include -arch x86_64 -Ipydensecrf/densecrf/include -Ipydensecrf -I/Users/hankmobley/opt/anaconda3/envs/liveproject/include/python3.7m -c pydensecrf/eigen.cpp -o build/temp.macosx-10.7-x86_64-3.7/pydensecrf/eigen.o\n",
      "    warning: include path for stdlibc++ headers not found; pass '-stdlib=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "    In file included from pydensecrf/eigen.cpp:644:\n",
      "    In file included from pydensecrf/densecrf/include/Eigen/Dense:1:\n",
      "    pydensecrf/densecrf/include/Eigen/Core:22:10: fatal error: 'complex' file not found\n",
      "    #include <complex>\n",
      "             ^~~~~~~~~\n",
      "    1 warning and 1 error generated.\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /Users/hankmobley/opt/anaconda3/envs/liveproject/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-req-build-y46axs3f/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/ft/5rj08hz110g9vqtvqf51hxqh0000gn/T/pip-record-8xog2kz4/install-record.txt --single-version-externally-managed --compile --install-headers /Users/hankmobley/opt/anaconda3/envs/liveproject/include/python3.7m/pydensecrf Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install cython\n",
    "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7zCYr6XyUPO"
   },
   "source": [
    "Import the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CO_sRLgOyUPP"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydensecrf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b5fdfedb8642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydensecrf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdensecrf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydensecrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munary_from_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydensecrf'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate, Conv2DTranspose, Flatten, Activation, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import json, os\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pydensecrf import densecrf\n",
    "from pydensecrf.utils import unary_from_labels\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bqp_9QXayUPQ"
   },
   "source": [
    "#### Define U-Resnet and CRF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp1tz-4syUPR"
   },
   "outputs": [],
   "source": [
    "def batchnorm_act(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation(\"relu\")(x)\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = batchnorm_act(x)\n",
    "    return Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
    "\n",
    "def bottleneck_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "    \n",
    "    bottleneck = Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    bottleneck = batchnorm_act(bottleneck)\n",
    "    \n",
    "    return Add()([conv, bottleneck])\n",
    "\n",
    "def res_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
    "    \n",
    "    bottleneck = Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    bottleneck = batchnorm_act(bottleneck)\n",
    "    \n",
    "    return Add()([bottleneck, res])\n",
    "\n",
    "def upsamp_concat_block(x, xskip):\n",
    "    u = UpSampling2D((2, 2))(x)\n",
    "    return Concatenate()([u, xskip])\n",
    "\n",
    "def res_unet(sz, f):\n",
    "    inputs = Input(sz)\n",
    "    \n",
    "    ## downsample  \n",
    "    e1 = bottleneck_block(inputs, f); f = int(f*2)\n",
    "    e2 = res_block(e1, f, strides=2); f = int(f*2)\n",
    "    e3 = res_block(e2, f, strides=2); f = int(f*2)\n",
    "    e4 = res_block(e3, f, strides=2); f = int(f*2)\n",
    "    _ = res_block(e4, f, strides=2)\n",
    "    \n",
    "    ## bottleneck\n",
    "    b0 = conv_block(_, f, strides=1)\n",
    "    _ = conv_block(b0, f, strides=1)\n",
    "    \n",
    "    ## upsample\n",
    "    _ = upsamp_concat_block(_, e4)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e3)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e2)\n",
    "    _ = res_block(_, f); f = int(f/2)\n",
    "    \n",
    "    _ = upsamp_concat_block(_, e1)\n",
    "    _ = res_block(_, f)\n",
    "    \n",
    "    ## classify\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(_)\n",
    "    \n",
    "    #model creation \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1YTg6kxyUPR"
   },
   "outputs": [],
   "source": [
    "def crf_labelrefine(input_image, predicted_labels):\n",
    "    \n",
    "    compat_spat=10\n",
    "    compat_col=30\n",
    "    theta_spat = 20\n",
    "    theta_col = 80\n",
    "    num_iter = 7\n",
    "    num_classes = 2\n",
    "    \n",
    "    h, w = input_image.shape[:2] #get image dimensions\n",
    "    \n",
    "    d = densecrf.DenseCRF2D(w, h, 2) #create a CRF object\n",
    "\n",
    "    # For the predictions, densecrf needs 'unary potentials' which are labels (water or no water)\n",
    "    predicted_unary = unary_from_labels(predicted_labels.astype('int')+1, num_classes, gt_prob= 0.51)\n",
    "    \n",
    "    # set the unary potentials to CRF object\n",
    "    d.setUnaryEnergy(predicted_unary)\n",
    "\n",
    "    # to add the color-independent term, where features are the locations only:\n",
    "    d.addPairwiseGaussian(sxy=(theta_spat, theta_spat), compat=compat_spat, kernel=densecrf.DIAG_KERNEL,\n",
    "                          normalization=densecrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    input_image_uint = (input_image * 255).astype(np.uint8) #enfore unsigned 8-bit\n",
    "    # to add the color-dependent term, i.e. 5-dimensional features are (x,y,r,g,b) based on the input image:    \n",
    "    d.addPairwiseBilateral(sxy=(theta_col, theta_col), srgb=(5, 5, 5), rgbim=input_image_uint,\n",
    "                           compat=compat_col, kernel=densecrf.DIAG_KERNEL, \n",
    "                           normalization=densecrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "    # Finally, we run inference to obtain the refined predictions:\n",
    "    refined_predictions = np.array(d.inference(num_iter)).reshape(num_classes, h, w)\n",
    "    \n",
    "    # since refined_predictions will be a 2 x width x height array, \n",
    "    # each slice respresenting probability of each class (water and no water)\n",
    "    # therefore we return the argmax over the zeroth dimension to return a mask\n",
    "    return np.argmax(refined_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSRA98Z6yUPR"
   },
   "source": [
    "#### Define Training functions (generators and callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVuq2A9UyUPS"
   },
   "outputs": [],
   "source": [
    "def image_batch_generator(files, batch_size = 32, sz = (512, 512)):\n",
    "  \n",
    "  while True: # this is here because it will be called repeatedly by the training function\n",
    "    \n",
    "    #extract a random subset of files of length \"batch_size\"\n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs (x) and outputs (y)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    #cycle through each image in the batch\n",
    "    for f in batch:\n",
    "\n",
    "        #preprocess the raw images \n",
    "        rawfile = f'nwpu_images/data/{f}'\n",
    "        raw = Image.open(rawfile)\n",
    "        raw = raw.resize(sz)\n",
    "        raw = np.array(raw)\n",
    "\n",
    "        #check the number of channels because some of the images are RGBA or GRAY\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.stack((raw,)*3, axis=-1)\n",
    "\n",
    "        else:\n",
    "            raw = raw[:,:,0:3]\n",
    "            \n",
    "        #get the image dimensions, find the min dimension, then square the image off    \n",
    "        nx, ny, nz = np.shape(raw)\n",
    "        n = np.minimum(nx,ny)\n",
    "        raw = raw[:n,:n,:] \n",
    "            \n",
    "        batch_x.append(raw)\n",
    "        \n",
    "        #get the masks. \n",
    "        maskfile = rawfile.replace('nwpu_images','nwpu_label_images')+'_mask.jpg'\n",
    "        mask = Image.open(maskfile)\n",
    "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
    "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
    "        # water pixels are always greater than 100\n",
    "        mask = (mask>200).astype('int')\n",
    "        \n",
    "        mask = mask[:n,:n]\n",
    "        \n",
    "        # use CRF to refine mask before it is used as a label\n",
    "        mask = crf_labelrefine(raw, mask).squeeze()\n",
    "\n",
    "        batch_y.append(mask)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
    "    batch_y = np.array(batch_y)\n",
    "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
    "\n",
    "    yield (batch_x, batch_y) #yield both the image and the label together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyxXniEmyUPT"
   },
   "outputs": [],
   "source": [
    "# inheritance for training process plot \n",
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        #self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('dice_coef'))\n",
    "        self.val_acc.append(logs.get('val_dice_coef'))\n",
    "        self.i += 1\n",
    "        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'dice_coef=',logs.get('dice_coef'),'val_dice_coef=',logs.get('val_dice_coef'))\n",
    "        \n",
    "        #choose a random test image and preprocess\n",
    "        path = np.random.choice(test_files)\n",
    "        infile = f's2cloudless_imagery/data/{path}'\n",
    "        raw = Image.open(infile)\n",
    "        raw = np.array(raw.resize((512, 512)))/255.\n",
    "        raw = raw[:,:,0:3]\n",
    "        \n",
    "        #predict the mask \n",
    "        pred = 255*model.predict(np.expand_dims(raw, 0)).squeeze()\n",
    "        print(np.max(pred))\n",
    "                \n",
    "        #mask post-processing \n",
    "        msk  = (pred>60).astype('int') #100    \n",
    "        \n",
    "        # use CRF to refine mask before it is used as a label\n",
    "        msk = crf_labelrefine(raw, msk).squeeze()\n",
    "      \n",
    "        msk = np.stack((msk,)*3, axis=-1)\n",
    "          \n",
    "        #show the mask and the segmented image \n",
    "        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(combined)\n",
    "        plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fORoCGc3yUPU"
   },
   "outputs": [],
   "source": [
    "def build_callbacks(filepath, min_delta, min_lr, factor):\n",
    "\n",
    "    earlystop = EarlyStopping(monitor=\"val_loss\", \n",
    "                                  mode=\"min\", patience=5) \n",
    "    \n",
    "    # reduction of learning rate if and when validation scores plateau upon successive epochs\n",
    "    reduceloss_plat = ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=5, \n",
    "                                    verbose=1, mode='auto', min_delta=min_delta, \n",
    "                                    cooldown=5, min_lr=min_lr)\n",
    "\n",
    "    # set checkpoint file \n",
    "    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                                   verbose=0, save_best_only=True, mode='min', \n",
    "                                   save_weights_only = True)\n",
    "        \n",
    "    callbacks = [model_checkpoint, reduceloss_plat, earlystop, PlotLearning()]\n",
    "\n",
    "    return callbacks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FusacgylyUPW"
   },
   "source": [
    "#### Define the loss function and compile the ResidualUNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kms4M9KbyUPW"
   },
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = tf.reshape(tf.dtypes.cast(y_true, tf.float32), [-1])\n",
    "    y_pred_f = tf.reshape(tf.dtypes.cast(y_pred, tf.float32), [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dywrMFYyUPW"
   },
   "outputs": [],
   "source": [
    "model = res_unet((512, 512, 3), 32)\n",
    "model.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGxP5XV-yUPW"
   },
   "source": [
    "#### Define training hyper-parameters, test/train files and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNRFILvzyUPX"
   },
   "outputs": [],
   "source": [
    "#split into training and testing\n",
    "train_files = os.listdir('nwpu_images/data')\n",
    "test_files  = os.listdir('s2cloudless_imagery/data')\n",
    "\n",
    "# a tolerance for the training. \n",
    "min_delta = 0.0001\n",
    "\n",
    "# minimum learning rate (lambda)\n",
    "min_lr = 0.0001\n",
    "\n",
    "# the factor applied to the learning rate when the appropriate triggers are made\n",
    "factor = 0.8\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "filepath = 'res_dice_crf_unet'+str(batch_size)+'.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiI7A_25yUPX"
   },
   "source": [
    "#### Create file generators and run model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNSV038KyUPY"
   },
   "source": [
    "#### Test NWPU model on S2 imagery\n",
    "\n",
    "First, set up a new generator function to generate batches of S2 augmented imagery and associated labels. Then, define a function that will use the model (trained on NWPU imagery) to estimate the binary semantic segmentation mask for 100 images. Finally, we'll look at the mean IOU score, and their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rfhb8w5yUPZ"
   },
   "outputs": [],
   "source": [
    "def image_batch_generatorS2(files, batch_size = 32, sz = (512, 512)):\n",
    "  \n",
    "  while True: # this is here because it will be called repeatedly by the training function\n",
    "    \n",
    "    #extract a random subset of files of length \"batch_size\"\n",
    "    batch = np.random.choice(files, size = batch_size)    \n",
    "    \n",
    "    #variables for collecting batches of inputs (x) and outputs (y)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    \n",
    "    #cycle through each image in the batch\n",
    "    for f in batch:\n",
    "\n",
    "        #preprocess the raw images \n",
    "        rawfile = f's2cloudless_imagery/data/{f}'\n",
    "        raw = Image.open(rawfile)\n",
    "        raw = raw.resize(sz)\n",
    "        raw = np.array(raw)\n",
    "\n",
    "        #check the number of channels because some of the images are RGBA or GRAY\n",
    "        if len(raw.shape) == 2:\n",
    "            raw = np.stack((raw,)*3, axis=-1)\n",
    "\n",
    "        else:\n",
    "            raw = raw[:,:,0:3]\n",
    "            \n",
    "        #get the image dimensions, find the min dimension, then square the image off    \n",
    "        nx, ny, nz = np.shape(raw)\n",
    "        n = np.minimum(nx,ny)\n",
    "        raw = raw[:n,:n,:] \n",
    "            \n",
    "        batch_x.append(raw)\n",
    "        \n",
    "        #get the masks. \n",
    "        maskfile = rawfile.replace('s2cloudless_imagery','s2cloudless_label_imagery')+'_mask.jpg'\n",
    "        mask = Image.open(maskfile)\n",
    "        # the mask is 3-dimensional so get the max in each channel to flatten to 2D\n",
    "        mask = np.max(np.array(mask.resize(sz)),axis=2)\n",
    "        # water pixels are always greater than 100\n",
    "        mask = (mask>100).astype('int')\n",
    "\n",
    "        # use CRF to refine mask before it is used as a label\n",
    "        mask = crf_labelrefine(raw, mask).squeeze()\n",
    "    \n",
    "        mask = mask[:n,:n]\n",
    "\n",
    "        batch_y.append(mask)\n",
    "\n",
    "    #preprocess a batch of images and masks \n",
    "    batch_x = np.array(batch_x)/255. #divide image by 255 to normalize\n",
    "    batch_y = np.array(batch_y)\n",
    "    batch_y = np.expand_dims(batch_y,3) #add singleton dimension to batch_y\n",
    "\n",
    "    yield (batch_x, batch_y) #yield both the image and the label together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMRVuUgTyUPa",
    "outputId": "de516b83-f138-4632-fc8d-711a3c8c14e0"
   },
   "outputs": [],
   "source": [
    "train_generator = image_batch_generator(train_files, batch_size = batch_size)\n",
    "test_generator  = image_batch_generatorS2(test_files, batch_size = batch_size)\n",
    "train_steps = len(train_files) //batch_size\n",
    "test_steps = len(test_files) //batch_size\n",
    "print(train_steps)\n",
    "print(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dCUPqWuLyUPb",
    "outputId": "8fe3065e-406b-4e8d-e884-116671fbef1c"
   },
   "outputs": [],
   "source": [
    "if load_res_dice_mix_history:\n",
    "  file_id = '116D4NpWzp_lXksXBD7kgL3KD_MFDNBLF'\n",
    "  destination = 'opt_model_7_res_dice_mix_hist'\n",
    "  if colab == 1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "  with open(\"opt_model_7_res_dice_mix_hist\", \"rb\") as file_pi:\n",
    "    res_dice_history = pickle.load(file_pi)\n",
    "  file_id = '1lH_fSh3AV-vC3W1kCmLGUeXr51ngxvZ_'\n",
    "  destination = 'opt_model_7_res_dice_mix'\n",
    "  if colab == 1:\n",
    "    download_file_from_google_drive(file_id, destination)\n",
    "  model = tf.keras.models.load_model('opt_model_7_res_dice_mix')\n",
    "elif colab == 1:\n",
    "  res_dice_history = model.fit_generator(train_generator,\n",
    "                                         epochs = 100, \n",
    "                                         steps_per_epoch = train_steps,\n",
    "                                         validation_data = test_generator, \n",
    "                                         validation_steps = test_steps,\n",
    "                                         callbacks = build_callbacks(filepath, \n",
    "                                                                     min_delta, \n",
    "                                                                     min_lr, \n",
    "                                                                     factor), \n",
    "                                         verbose = 0,\n",
    "                                         use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DAGzFpsw_dZ"
   },
   "outputs": [],
   "source": [
    "if load_res_dice_mix_history == 0 and colab == 1:\n",
    "  #save model and history to load or graph later\n",
    "  model.save(\"opt_model_7_res_dice_mix\",save_format=\"h5\")\n",
    "  \n",
    "  with open('opt_model_7_res_dice_mix_hist', 'wb') as file_pi:\n",
    "    pickle.dump(res_dice_history.history, file_pi)# save model and history to load or graph later\n",
    "  \n",
    "  #del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOiJZMdQyUPb"
   },
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    yt0 = y_true.squeeze()\n",
    "    yp0 = tf.keras.backend.cast(y_pred.squeeze() > 0.5, 'float32')\n",
    "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
    "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
    "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
    "    return iou\n",
    "\n",
    "# a function for getting a CRF-refined estimated water mask from an input image, \n",
    "# and IOU score evaluated against the real mask\n",
    "def get_pred(x, y):\n",
    "    #predict the mask \n",
    "    pred = model.predict(np.expand_dims(x, 0))\n",
    "    \n",
    "    #mask post-processing \n",
    "    msk  = pred.squeeze()\n",
    "    # binarize\n",
    "    msk[msk >= 0.5] = 1 \n",
    "    msk[msk < 0.5] = 0\n",
    "    \n",
    "    # use CRF to refine mask before it is used as a label\n",
    "    msk = crf_labelrefine(x.squeeze(), msk).squeeze()\n",
    "        \n",
    "    # return the prediction and the IOU score of the prediction\n",
    "    return msk, mean_iou(y, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm6CqUxOyUPc"
   },
   "outputs": [],
   "source": [
    "all_files = os.listdir('s2cloudless_imagery/data')\n",
    "shuffle(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8giFxKJNyUPd"
   },
   "source": [
    "Let's see how this will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "4n67q1GtyUPe",
    "outputId": "e01fdb31-ef35-4e11-c720-8a04afe86d17"
   },
   "outputs": [],
   "source": [
    "# run the S2 generator function\n",
    "test_generatorS2  = image_batch_generatorS2(all_files, batch_size = batch_size)\n",
    "\n",
    "#get a batch of S2 imagery and labels\n",
    "x, y = next(test_generatorS2) \n",
    "#get the predicted mask and iou score for the first\n",
    "ypred, iou = get_pred(x[0], y[0]) \n",
    "\n",
    "#make a plot side-by-side of label ...\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(y[0].squeeze(), alpha=0.5, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# ... and estimated label\n",
    "plt.subplot(122)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(ypred.squeeze(), alpha=0.5, cmap='gray')\n",
    "plt.title('IoU = '+str(iou.numpy())[:4])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cU1-_lwyUPf"
   },
   "source": [
    "For up to 100 images, make a prediction and store the mean iou statistic. Print a validation example every 5th image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5IbEhUSnyUPf",
    "outputId": "229e301c-0d27-47e8-aa4c-81d44f8d0c99"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "IOU = [] #initialize list\n",
    "counter = 0 #initialize counter\n",
    "while counter < 100: # compare 100 images\n",
    "    x, y = next(test_generatorS2)\n",
    "    ypred, iou = get_pred(x[0], y[0])\n",
    "    IOU.append(iou) #update list\n",
    "\n",
    "    if counter % 5 == 0: #print every 5th comparison\n",
    "        plt.figure(figsize=(4,6))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(x[0])\n",
    "        plt.imshow(y[0].squeeze(), alpha=0.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(x[0])\n",
    "        plt.imshow(ypred.squeeze(), alpha=0.5, cmap='gray')\n",
    "        plt.title(str(iou.numpy())[:4])\n",
    "        plt.axis('off')\n",
    "        print(counter)  \n",
    "        \n",
    "    counter += 1 #update counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04qhvHv3yUPj"
   },
   "source": [
    "Print the median IOU score to screen and make a histogram plot of the distribution of IOU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prA8kvFYyUPk",
    "outputId": "165f451d-dede-45c4-8e75-8ad3bb851c6f"
   },
   "outputs": [],
   "source": [
    "print(np.median(IOU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6lNLDcnyUPl"
   },
   "source": [
    "There are 3 peaks in the histogram; the first at IoU ~ 0 where the algorithm didn't detect a lake at all (about 10% of samples), the second around ~0.5 where only one lake out of many were detected, and finally the biggest peak around ~0.95 accoting for about 70% of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "yx64DG3xyUPm",
    "outputId": "4c25ceb6-60ba-4b23-f356-c15771b46428"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fliers = dict(markerfacecolor='g', marker='p')\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(IOU)\n",
    "plt.xlabel('IOU scores')\n",
    "plt.axvline(np.median(IOU), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwH-vP8FyUPm"
   },
   "source": [
    "#### Combine two models in an ensemble prediction\n",
    "\n",
    "Create two models and use the ```load_weights``` utility to load the weights contained in the h5 files (the product of model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNGDe5f264BA"
   },
   "outputs": [],
   "source": [
    "#file_id = '1BEw63yYh1Wt6Dbz85gm-U1wmi2Nyddbo'\n",
    "#destination = 'opt_model_5_res_dice'\n",
    "#if colab==1:\n",
    "#    download_file_from_google_drive(file_id, destination)\n",
    "file_id = '1QgJqz2el-9it4e9rAUUeQgWMpZbcMlKL'\n",
    "destination = 'opt_model_6_res_dice2'\n",
    "if colab==1:\n",
    "    download_file_from_google_drive(file_id, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlKlKDx-7SW_"
   },
   "outputs": [],
   "source": [
    "if colab==1:\n",
    "  m1 = tf.keras.models.load_model('opt_model_6_res_dice2',\n",
    "                                  custom_objects = {\"dice_coef_loss\":dice_coef_loss,\n",
    "                                                    \"dice_coef\":dice_coef})\n",
    "#m2 = tf.keras.models.load_model('opt_model_7_res_dice_mix',\n",
    "#                               custom_objects = {\"dice_coef_loss\":dice_coef_loss,\n",
    "#                                                \"dice_coef\":dice_coef})\n",
    "#m2 = tf.keras.models.load_model('opt_model_5_res_dice',\n",
    "#                               custom_objects = {\"dice_coef_loss\":dice_coef_loss,\n",
    "#                                                \"dice_coef\":dice_coef})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKV0OTTH8AEr"
   },
   "outputs": [],
   "source": [
    "if colab==1:\n",
    "    #!mkdir weights\n",
    "    m1.save_weights('res_dice_crf_unet1.h5')\n",
    "    model.save_weights('res_dice_crf_unet2.h5')\n",
    "    #m2.save_weights('res_dice_unet2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45vicqp5yUPm"
   },
   "outputs": [],
   "source": [
    "model1 = res_unet((512, 512, 3), 32)\n",
    "model1.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model1.load_weights('res_dice_crf_unet1.h5')\n",
    "\n",
    "model2 = res_unet((512, 512, 3), 32)\n",
    "model2.compile(optimizer = 'rmsprop', loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model2.load_weights('res_dice_crf_unet2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZSt5KbdyUPm"
   },
   "source": [
    "Now update the `get_pred` function to use the maximum of the two model outputs on an input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtaB1UGwyUPn"
   },
   "outputs": [],
   "source": [
    "# a function for getting a CRF-refined estimated water mask from an input image, \n",
    "# and IOU score evaluated against the real mask\n",
    "def get_pred(x, y, model1, model2):\n",
    "    #predict the mask \n",
    "    pred1 = model1.predict(np.expand_dims(x, 0))\n",
    "    pred2 = model2.predict(np.expand_dims(x, 0))\n",
    "    \n",
    "    #mask post-processing \n",
    "    msk  = np.maximum(pred1.squeeze(), pred2.squeeze())\n",
    "    # binarize\n",
    "    msk[msk >= 0.5] = 1 \n",
    "    msk[msk < 0.5] = 0\n",
    "    \n",
    "    # use CRF to refine mask before it is used as a label\n",
    "    msk = crf_labelrefine(x.squeeze(), msk).squeeze()\n",
    "        \n",
    "    # return the prediction and the IOU score of the prediction\n",
    "    return msk, mean_iou(y, msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJefRrHpyUPo"
   },
   "source": [
    "Test things, like you did earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "Ar8BepjJyUPo",
    "outputId": "373902b7-f7eb-427e-b3ba-13e68a3e280b"
   },
   "outputs": [],
   "source": [
    "# run the S2 generator function\n",
    "test_generatorS2  = image_batch_generatorS2(all_files, batch_size = batch_size)\n",
    "\n",
    "#get a batch of S2 imagery and labels\n",
    "x, y = next(test_generatorS2) \n",
    "#get the predicted mask and iou score for the first\n",
    "ypred, iou = get_pred(x[0], y[0], model1, model2) \n",
    "\n",
    "#make a plot side-by-side of label ...\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(y[0].squeeze(), alpha=0.5, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# ... and estimated label\n",
    "plt.subplot(122)\n",
    "plt.imshow(x[0])\n",
    "plt.imshow(ypred.squeeze(), alpha=0.5, cmap='gray')\n",
    "plt.title('IoU = '+str(iou.numpy())[:4])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsfaTwAtyUPo"
   },
   "source": [
    "Just like before, for up to 100 images, make a prediction using both models, and store the mean iou statistic. Print a validation example every 5th image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3iRPzeD2yUPp",
    "outputId": "75a47bd1-73f2-4a10-9cef-62f3c132f409"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "IOU = [] #initialize list\n",
    "counter = 0 #initialize counter\n",
    "while counter < 100: # compare 100 images\n",
    "    x, y = next(test_generatorS2)\n",
    "    ypred, iou = get_pred(x[0], y[0], model1, model2)\n",
    "    IOU.append(iou) #update list\n",
    "\n",
    "    if counter % 5 == 0: #print every 5th comparison\n",
    "        plt.figure(figsize=(4,6))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(x[0])\n",
    "        plt.imshow(y[0].squeeze(), alpha=0.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(x[0])\n",
    "        plt.imshow(ypred.squeeze(), alpha=0.5, cmap='gray')\n",
    "        plt.title(str(iou.numpy())[:4])\n",
    "        plt.axis('off')\n",
    "        print(counter)  \n",
    "        \n",
    "    counter += 1 #update counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaEZ_WKkyUPq"
   },
   "source": [
    "compute the median, and show the histograms of IOUs. As you can see, ensemble predictions are a very simple way to improve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUW6RryeyUPq",
    "outputId": "d53ea33c-b4f2-4477-b5bd-8f83e8cab9a8"
   },
   "outputs": [],
   "source": [
    "print(np.median(IOU))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "GvQeU6S-yUPr",
    "outputId": "9175ddb6-a445-4448-e653-e9a894146469"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fliers = dict(markerfacecolor='g', marker='p')\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(IOU)\n",
    "plt.xlabel('IOU scores')\n",
    "plt.axvline(np.median(IOU), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IamwFHG9yUPr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "5_Model_Optimization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
